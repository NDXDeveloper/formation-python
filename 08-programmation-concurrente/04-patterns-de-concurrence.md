üîù Retour au [Sommaire](/SOMMAIRE.md)

# 8.4 : Patterns de concurrence

## Introduction

Les patterns de concurrence sont des solutions √©prouv√©es pour r√©soudre des probl√®mes r√©currents en programmation parall√®le. Ces mod√®les vous aident √† structurer vos applications concurrentes de mani√®re efficace et maintenable.

### Analogie simple
Imaginez les patterns comme des **recettes de cuisine √©prouv√©es** :
- **Producer-Consumer** : Une cha√Æne de production (fabricant ‚Üí convoyeur ‚Üí emballeur)
- **Worker Pool** : Une √©quipe de cuisiniers qui se partagent les commandes
- **Pipeline** : Une cha√Æne de montage o√π chaque station a une fonction
- **Map-Reduce** : Diviser une grosse t√¢che, traiter en parall√®le, puis combiner

## Pattern Producer-Consumer

Le pattern le plus fondamental : des producteurs cr√©ent des donn√©es, des consommateurs les traitent.

### Implementation basique

```python
import threading
import queue
import time
import random

class ProducerConsumer:
    """Implementation du pattern Producer-Consumer."""

    def __init__(self, queue_size=10):
        self.queue = queue.Queue(maxsize=queue_size)
        self.active = True
        self.stats = {
            'produced': 0,
            'consumed': 0,
            'errors': 0
        }
        self.stats_lock = threading.Lock()

    def producer(self, name, items_count, delay_range=(0.1, 0.5)):
        """Produit des √©l√©ments √† intervalles r√©guliers."""
        print(f"üè≠ {name} d√©marr√© (production de {items_count} √©l√©ments)")

        for i in range(items_count):
            if not self.active:
                break

            # Cr√©er un √©l√©ment
            item = {
                'id': f"{name}_{i+1}",
                'data': random.randint(1, 100),
                'timestamp': time.time(),
                'producer': name
            }

            try:
                # Ajouter √† la queue (bloque si pleine)
                self.queue.put(item, timeout=2)

                with self.stats_lock:
                    self.stats['produced'] += 1

                print(f"üì¶ {name}: Produit {item['id']} (data={item['data']})")

                # D√©lai variable
                time.sleep(random.uniform(*delay_range))

            except queue.Full:
                print(f"‚ö†Ô∏è {name}: Queue pleine, √©l√©ment perdu")
                with self.stats_lock:
                    self.stats['errors'] += 1

        print(f"‚úÖ {name}: Production termin√©e")

    def consumer(self, name, processing_time_range=(0.2, 0.8)):
        """Consomme et traite des √©l√©ments."""
        print(f"üîÑ {name} d√©marr√©")

        while self.active:
            try:
                # R√©cup√©rer un √©l√©ment (bloque si vide)
                item = self.queue.get(timeout=1)

                print(f"üì• {name}: Traite {item['id']}")

                # Simuler le traitement
                processing_time = random.uniform(*processing_time_range)
                time.sleep(processing_time)

                # Marquer comme termin√©
                self.queue.task_done()

                with self.stats_lock:
                    self.stats['consumed'] += 1

                print(f"‚úÖ {name}: Termin√© {item['id']} ({processing_time:.2f}s)")

            except queue.Empty:
                # Timeout, v√©rifier si on doit continuer
                continue
            except Exception as e:
                print(f"‚ùå {name}: Erreur - {e}")
                with self.stats_lock:
                    self.stats['errors'] += 1

        print(f"üõë {name}: Arr√™t√©")

    def run(self, producers_config, consumers_config, duration=10):
        """Lance le syst√®me producer-consumer."""
        print("üöÄ D√©marrage du syst√®me Producer-Consumer")
        print(f"Producteurs: {len(producers_config)}, Consommateurs: {len(consumers_config)}")
        print(f"Dur√©e: {duration}s\n")

        threads = []

        # D√©marrer les producteurs
        for name, items_count in producers_config:
            t = threading.Thread(target=self.producer, args=(name, items_count))
            t.start()
            threads.append(t)

        # D√©marrer les consommateurs
        for name in consumers_config:
            t = threading.Thread(target=self.consumer, args=(name,))
            t.start()
            threads.append(t)

        # Laisser fonctionner
        time.sleep(duration)

        # Arr√™ter le syst√®me
        print("\nüõë Arr√™t du syst√®me...")
        self.active = False

        # Attendre les producteurs
        for t in threads[:len(producers_config)]:
            t.join()

        # Vider la queue restante
        try:
            while True:
                self.queue.get_nowait()
                self.queue.task_done()
        except queue.Empty:
            pass

        # Attendre les consommateurs
        for t in threads[len(producers_config):]:
            t.join(timeout=2)

        self.print_stats()

    def print_stats(self):
        """Affiche les statistiques finales."""
        with self.stats_lock:
            stats = self.stats.copy()

        print(f"\nüìä STATISTIQUES FINALES")
        print(f"Produits: {stats['produced']}")
        print(f"Consomm√©s: {stats['consumed']}")
        print(f"Erreurs: {stats['errors']}")
        print(f"Queue restante: {self.queue.qsize()}")

# Test du pattern
def demo_producer_consumer():
    """D√©monstration du pattern Producer-Consumer."""
    pc = ProducerConsumer(queue_size=5)

    producers = [("Prod-A", 8), ("Prod-B", 6)]
    consumers = ["Cons-1", "Cons-2", "Cons-3"]

    pc.run(producers, consumers, duration=12)

demo_producer_consumer()
```

## Pattern Worker Pool

Pool de workers qui se partagent les t√¢ches d'une queue commune.

### Worker Pool avec ThreadPoolExecutor

```python
import concurrent.futures
import time
import random

def tache_worker(task_data):
    """Fonction ex√©cut√©e par un worker."""
    task_id = task_data['id']
    complexity = task_data['complexity']

    print(f"üîÑ Worker traite t√¢che {task_id} (complexit√©: {complexity})")

    # Simuler le travail (plus c'est complexe, plus c'est long)
    work_time = complexity * random.uniform(0.1, 0.3)
    time.sleep(work_time)

    # Simuler des erreurs occasionnelles
    if random.random() < 0.1:
        raise Exception(f"Erreur simul√©e pour t√¢che {task_id}")

    result = {
        'task_id': task_id,
        'result': f"R√©sultat_{task_id}",
        'work_time': work_time,
        'worker_thread': threading.current_thread().name
    }

    print(f"‚úÖ T√¢che {task_id} termin√©e ({work_time:.2f}s)")
    return result

class WorkerPool:
    """Gestionnaire de pool de workers."""

    def __init__(self, max_workers=4):
        self.max_workers = max_workers

    def process_tasks(self, tasks):
        """Traite une liste de t√¢ches avec le pool."""
        print(f"üèä Pool de {self.max_workers} workers pour {len(tasks)} t√¢ches")

        results = []
        errors = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Soumettre toutes les t√¢ches
            future_to_task = {
                executor.submit(tache_worker, task): task
                for task in tasks
            }

            # R√©cup√©rer les r√©sultats au fur et √† mesure
            for future in concurrent.futures.as_completed(future_to_task):
                task = future_to_task[future]

                try:
                    result = future.result()
                    results.append(result)

                except Exception as e:
                    error_info = {
                        'task_id': task['id'],
                        'error': str(e)
                    }
                    errors.append(error_info)
                    print(f"‚ùå Erreur t√¢che {task['id']}: {e}")

        return results, errors

    def process_with_timeout(self, tasks, timeout_per_task=5):
        """Traite avec timeout par t√¢che."""
        print(f"‚è∞ Pool avec timeout de {timeout_per_task}s par t√¢che")

        results = []
        errors = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Soumettre toutes les t√¢ches
            futures = [executor.submit(tache_worker, task) for task in tasks]

            # Attendre avec timeout
            for i, future in enumerate(futures):
                try:
                    result = future.result(timeout=timeout_per_task)
                    results.append(result)

                except concurrent.futures.TimeoutError:
                    future.cancel()
                    error_info = {
                        'task_id': tasks[i]['id'],
                        'error': 'Timeout'
                    }
                    errors.append(error_info)
                    print(f"‚è∞ Timeout t√¢che {tasks[i]['id']}")

                except Exception as e:
                    error_info = {
                        'task_id': tasks[i]['id'],
                        'error': str(e)
                    }
                    errors.append(error_info)

        return results, errors

def demo_worker_pool():
    """D√©monstration du pattern Worker Pool."""
    print("üë• PATTERN WORKER POOL")
    print("-" * 25)

    # Cr√©er des t√¢ches vari√©es
    tasks = [
        {'id': f'task_{i}', 'complexity': random.randint(1, 5)}
        for i in range(1, 13)
    ]

    pool = WorkerPool(max_workers=3)

    start_time = time.time()
    results, errors = pool.process_tasks(tasks)
    total_time = time.time() - start_time

    print(f"\nüìä R√âSULTATS")
    print(f"T√¢ches r√©ussies: {len(results)}")
    print(f"Erreurs: {len(errors)}")
    print(f"Temps total: {total_time:.2f}s")

    # Afficher qui a fait quoi
    worker_stats = {}
    for result in results:
        worker = result['worker_thread']
        worker_stats[worker] = worker_stats.get(worker, 0) + 1

    print(f"\nR√©partition par worker:")
    for worker, count in worker_stats.items():
        print(f"  {worker}: {count} t√¢ches")

demo_worker_pool()
```

## Pattern Pipeline

Traitement en pipeline o√π chaque √©tape transforme les donn√©es.

### Pipeline de traitement d'images

```python
import threading
import queue
import time
import random

class PipelineStage:
    """√âtape g√©n√©rique d'un pipeline."""

    def __init__(self, name, process_func, input_queue=None, output_queue=None):
        self.name = name
        self.process_func = process_func
        self.input_queue = input_queue or queue.Queue()
        self.output_queue = output_queue
        self.active = True
        self.stats = {'processed': 0, 'errors': 0}

    def run(self):
        """Ex√©cute l'√©tape du pipeline."""
        print(f"üîß {self.name} d√©marr√©")

        while self.active:
            try:
                # R√©cup√©rer un √©l√©ment √† traiter
                item = self.input_queue.get(timeout=1)

                if item is None:  # Signal d'arr√™t
                    break

                # Traiter l'√©l√©ment
                print(f"üîÑ {self.name}: Traite {item.get('id', 'item')}")

                try:
                    result = self.process_func(item)

                    # Envoyer √† l'√©tape suivante
                    if self.output_queue:
                        self.output_queue.put(result)

                    self.stats['processed'] += 1
                    print(f"‚úÖ {self.name}: Termin√© {item.get('id', 'item')}")

                except Exception as e:
                    print(f"‚ùå {self.name}: Erreur - {e}")
                    self.stats['errors'] += 1

                self.input_queue.task_done()

            except queue.Empty:
                continue

        print(f"üõë {self.name} arr√™t√©")

class ImagePipeline:
    """Pipeline de traitement d'images."""

    def __init__(self):
        # Queues entre les √©tapes
        self.queue_load = queue.Queue()
        self.queue_resize = queue.Queue()
        self.queue_filter = queue.Queue()
        self.queue_save = queue.Queue()

        # √âtapes du pipeline
        self.stages = [
            PipelineStage("Loader", self.load_image, self.queue_load, self.queue_resize),
            PipelineStage("Resizer", self.resize_image, self.queue_resize, self.queue_filter),
            PipelineStage("Filter", self.apply_filter, self.queue_filter, self.queue_save),
            PipelineStage("Saver", self.save_image, self.queue_save, None)
        ]

        self.threads = []

    def load_image(self, request):
        """√âtape 1: Charger l'image."""
        time.sleep(random.uniform(0.1, 0.3))  # Simuler I/O

        return {
            'id': request['id'],
            'filename': request['filename'],
            'data': f"raw_data_{request['id']}",
            'stage': 'loaded'
        }

    def resize_image(self, image):
        """√âtape 2: Redimensionner."""
        time.sleep(random.uniform(0.2, 0.4))  # Simuler traitement

        image['data'] = f"resized_{image['data']}"
        image['stage'] = 'resized'
        return image

    def apply_filter(self, image):
        """√âtape 3: Appliquer un filtre."""
        time.sleep(random.uniform(0.1, 0.2))  # Simuler traitement

        image['data'] = f"filtered_{image['data']}"
        image['stage'] = 'filtered'
        return image

    def save_image(self, image):
        """√âtape 4: Sauvegarder."""
        time.sleep(random.uniform(0.1, 0.3))  # Simuler I/O

        image['data'] = f"saved_{image['data']}"
        image['stage'] = 'saved'
        return image

    def start(self):
        """D√©marre le pipeline."""
        print("üöÄ D√©marrage du pipeline de traitement")

        # D√©marrer chaque √©tape dans son propre thread
        for stage in self.stages:
            thread = threading.Thread(target=stage.run)
            thread.start()
            self.threads.append(thread)

        print("‚úÖ Pipeline d√©marr√©")

    def process_images(self, image_requests):
        """Traite une liste de demandes d'images."""
        print(f"üì∏ Traitement de {len(image_requests)} images")

        # Injecter les demandes dans le pipeline
        for request in image_requests:
            self.queue_load.put(request)

        # Attendre que toutes les images soient charg√©es
        self.queue_load.join()

        # Attendre que toutes les √©tapes suivantes se terminent
        for queue in [self.queue_resize, self.queue_filter, self.queue_save]:
            queue.join()

    def stop(self):
        """Arr√™te le pipeline."""
        print("üõë Arr√™t du pipeline...")

        # Envoyer signal d'arr√™t √† chaque √©tape
        for stage in self.stages:
            stage.active = False
            stage.input_queue.put(None)  # Signal d'arr√™t

        # Attendre la fin des threads
        for thread in self.threads:
            thread.join(timeout=2)

        self.print_stats()

    def print_stats(self):
        """Affiche les statistiques du pipeline."""
        print(f"\nüìä STATISTIQUES DU PIPELINE")
        for stage in self.stages:
            print(f"{stage.name}: {stage.stats['processed']} trait√©s, {stage.stats['errors']} erreurs")

def demo_pipeline():
    """D√©monstration du pattern Pipeline."""
    print("üè≠ PATTERN PIPELINE")
    print("-" * 20)

    # Cr√©er des demandes d'images
    image_requests = [
        {'id': f'img_{i}', 'filename': f'photo_{i}.jpg'}
        for i in range(1, 9)
    ]

    pipeline = ImagePipeline()

    try:
        pipeline.start()
        time.sleep(1)  # Laisser le pipeline se stabiliser

        start_time = time.time()
        pipeline.process_images(image_requests)
        total_time = time.time() - start_time

        print(f"\n‚è±Ô∏è Traitement termin√© en {total_time:.2f}s")

    finally:
        pipeline.stop()

demo_pipeline()
```

## Pattern Map-Reduce

Diviser une t√¢che en sous-t√¢ches, les traiter en parall√®le, puis combiner les r√©sultats.

### Map-Reduce pour analyse de texte

```python
import concurrent.futures
import time
import random
from collections import defaultdict, Counter
import re

class MapReduce:
    """Impl√©mentation g√©n√©rique du pattern Map-Reduce."""

    def __init__(self, max_workers=4):
        self.max_workers = max_workers

    def map_phase(self, data_chunks, map_func):
        """Phase Map: traite les chunks en parall√®le."""
        print(f"üó∫Ô∏è Phase MAP: {len(data_chunks)} chunks avec {self.max_workers} workers")

        map_results = []

        with concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # Soumettre tous les chunks
            futures = [executor.submit(map_func, chunk) for chunk in data_chunks]

            # R√©cup√©rer les r√©sultats
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                try:
                    result = future.result()
                    map_results.append(result)
                    print(f"‚úÖ Chunk {i+1} trait√©")
                except Exception as e:
                    print(f"‚ùå Erreur chunk {i+1}: {e}")

        return map_results

    def reduce_phase(self, map_results, reduce_func):
        """Phase Reduce: combine les r√©sultats."""
        print(f"üîÑ Phase REDUCE: Combinaison de {len(map_results)} r√©sultats")

        if not map_results:
            return None

        # R√©duction s√©quentielle (peut √™tre parall√©lis√©e pour de gros volumes)
        result = map_results[0]
        for other_result in map_results[1:]:
            result = reduce_func(result, other_result)

        return result

    def execute(self, data, chunk_size, map_func, reduce_func):
        """Ex√©cute le processus Map-Reduce complet."""
        print(f"üöÄ D√âBUT MAP-REDUCE")
        print(f"Donn√©es: {len(data)} √©l√©ments, chunks de {chunk_size}")

        start_time = time.time()

        # 1. Diviser les donn√©es en chunks
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        print(f"üì¶ {len(chunks)} chunks cr√©√©s")

        # 2. Phase Map
        map_results = self.map_phase(chunks, map_func)

        # 3. Phase Reduce
        final_result = self.reduce_phase(map_results, reduce_func)

        total_time = time.time() - start_time
        print(f"‚è±Ô∏è Map-Reduce termin√© en {total_time:.2f}s")

        return final_result

# Fonctions pour l'analyse de texte
def map_word_count(text_chunk):
    """Fonction Map: compte les mots dans un chunk de texte."""
    word_count = defaultdict(int)

    for line in text_chunk:
        # Nettoyer et diviser en mots
        words = re.findall(r'\b\w+\b', line.lower())
        for word in words:
            word_count[word] += 1

    return dict(word_count)

def reduce_word_count(count1, count2):
    """Fonction Reduce: combine deux dictionnaires de comptage."""
    result = defaultdict(int)

    # Additionner les comptages
    for word, count in count1.items():
        result[word] += count

    for word, count in count2.items():
        result[word] += count

    return dict(result)

def demo_map_reduce():
    """D√©monstration du pattern Map-Reduce."""
    print("üåç PATTERN MAP-REDUCE")
    print("-" * 25)

    # Cr√©er un corpus de texte d'exemple
    sample_texts = [
        "Python est un langage de programmation puissant et facile √† apprendre.",
        "La programmation concurrente am√©liore les performances des applications.",
        "Les patterns de concurrence sont des solutions √©prouv√©es et efficaces.",
        "Python offre plusieurs outils pour la programmation parall√®le et asynchrone.",
        "Map-Reduce est un pattern populaire pour traiter de gros volumes de donn√©es.",
        "Les threads et les processus permettent d'ex√©cuter du code en parall√®le.",
        "La synchronisation est cruciale pour √©viter les conditions de course.",
        "Les queues facilitent la communication entre threads et processus.",
    ] * 100  # Multiplier pour avoir plus de donn√©es

    # Ajouter du bruit pour rendre plus r√©aliste
    for i in range(200):
        words = ["donn√©es", "traitement", "performance", "code", "syst√®me", "application"]
        sample_texts.append(" ".join(random.choices(words, k=random.randint(3, 8))))

    print(f"üìñ Corpus: {len(sample_texts)} lignes de texte")

    # Ex√©cuter Map-Reduce
    mr = MapReduce(max_workers=3)
    word_counts = mr.execute(
        data=sample_texts,
        chunk_size=100,
        map_func=map_word_count,
        reduce_func=reduce_word_count
    )

    # Analyser les r√©sultats
    if word_counts:
        total_words = sum(word_counts.values())
        unique_words = len(word_counts)

        print(f"\nüìä R√âSULTATS D'ANALYSE")
        print(f"Mots uniques: {unique_words}")
        print(f"Total des mots: {total_words}")

        # Top 10 des mots les plus fr√©quents
        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        print(f"\nTop 10 des mots:")
        for word, count in top_words:
            print(f"  {word}: {count}")

if __name__ == "__main__":
    demo_map_reduce()
```

## Pattern Circuit Breaker

Protection contre les d√©faillances en cascade en "coupant le circuit" quand un service est d√©faillant.

### Circuit Breaker pour services externes

```python
import threading
import time
import random
from enum import Enum
from datetime import datetime, timedelta

class CircuitState(Enum):
    CLOSED = "closed"      # Normal: requ√™tes passent
    OPEN = "open"          # D√©faillant: requ√™tes bloqu√©es
    HALF_OPEN = "half_open"  # Test: quelques requ√™tes passent

class CircuitBreaker:
    """Impl√©mentation du pattern Circuit Breaker."""

    def __init__(self, failure_threshold=5, timeout=60, expected_exception=Exception):
        self.failure_threshold = failure_threshold  # Seuil d'√©checs
        self.timeout = timeout  # Temps avant retry (secondes)
        self.expected_exception = expected_exception

        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

        self.lock = threading.Lock()
        self.stats = {
            'total_calls': 0,
            'successful_calls': 0,
            'failed_calls': 0,
            'circuit_open_calls': 0
        }

    def call(self, func, *args, **kwargs):
        """Ex√©cute une fonction prot√©g√©e par le circuit breaker."""
        with self.lock:
            self.stats['total_calls'] += 1

            # V√©rifier l'√©tat du circuit
            if self.state == CircuitState.OPEN:
                if self._should_attempt_reset():
                    self.state = CircuitState.HALF_OPEN
                    print(f"üîÑ Circuit HALF-OPEN: Test de r√©cup√©ration")
                else:
                    self.stats['circuit_open_calls'] += 1
                    print(f"‚ö° Circuit OPEN: Appel bloqu√©")
                    raise Exception("Circuit breaker OPEN")

            # Tenter l'appel
            try:
                result = func(*args, **kwargs)
                self._on_success()
                return result

            except self.expected_exception as e:
                self._on_failure()
                raise e

    def _should_attempt_reset(self):
        """V√©rifie si on peut tenter de r√©initialiser le circuit."""
        if self.last_failure_time is None:
            return True

        return datetime.now() - self.last_failure_time > timedelta(seconds=self.timeout)

    def _on_success(self):
        """Appel√© lors d'un succ√®s."""
        self.stats['successful_calls'] += 1

        if self.state == CircuitState.HALF_OPEN:
            self.state = CircuitState.CLOSED
            self.failure_count = 0
            print(f"‚úÖ Circuit CLOSED: Service r√©cup√©r√©")

    def _on_failure(self):
        """Appel√© lors d'un √©chec."""
        self.stats['failed_calls'] += 1
        self.failure_count += 1
        self.last_failure_time = datetime.now()

        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN
            print(f"‚ö° Circuit OPEN: Trop d'√©checs ({self.failure_count})")

    def get_stats(self):
        """Retourne les statistiques."""
        with self.lock:
            stats = self.stats.copy()
            stats['state'] = self.state.value
            stats['failure_count'] = self.failure_count
            return stats

class ServiceExterne:
    """Simule un service externe instable."""

    def __init__(self, nom, taux_echec=0.3):
        self.nom = nom
        self.taux_echec = taux_echec
        self.appels = 0

    def appeler_api(self, donnees):
        """Simule un appel API qui peut √©chouer."""
        self.appels += 1

        # Simuler latence r√©seau
        time.sleep(random.uniform(0.1, 0.3))

        # Simuler des √©checs
        if random.random() < self.taux_echec:
            raise Exception(f"{self.nom}: Service temporairement indisponible")

        return f"{self.nom}: Donn√©es trait√©es - {donnees}"

def demo_circuit_breaker():
    """D√©monstration du pattern Circuit Breaker."""
    print("‚ö° PATTERN CIRCUIT BREAKER")
    print("-" * 30)

    # Cr√©er un service instable
    service = ServiceExterne("API-Externe", taux_echec=0.4)

    # Cr√©er un circuit breaker
    circuit = CircuitBreaker(
        failure_threshold=3,
        timeout=5,
        expected_exception=Exception
    )

    # Simuler des appels clients
    def client_worker(nom_client, nb_appels):
        """Simule un client qui fait des appels."""
        print(f"üë§ {nom_client} d√©marr√©")

        for i in range(nb_appels):
            try:
                donnees = f"requete_{nom_client}_{i+1}"
                result = circuit.call(service.appeler_api, donnees)
                print(f"‚úÖ {nom_client}: {result}")

            except Exception as e:
                print(f"‚ùå {nom_client}: {e}")

            time.sleep(random.uniform(0.5, 1.5))

        print(f"üèÅ {nom_client} termin√©")

   # Lancer plusieurs clients
    clients = ["Client-A", "Client-B", "Client-C"]
    threads = []

    for client in clients:
        t = threading.Thread(target=client_worker, args=(client, 8))
        t.start()
        threads.append(t)

    # Attendre la fin
    for t in threads:
        t.join()

    # Afficher les statistiques finales
    stats = circuit.get_stats()
    print(f"\nüìä STATISTIQUES CIRCUIT BREAKER")
    print(f"√âtat final: {stats['state'].upper()}")
    print(f"Appels totaux: {stats['total_calls']}")
    print(f"Succ√®s: {stats['successful_calls']}")
    print(f"√âchecs: {stats['failed_calls']}")
    print(f"Bloqu√©s (circuit ouvert): {stats['circuit_open_calls']}")
    print(f"Taux de succ√®s: {stats['successful_calls']/stats['total_calls']*100:.1f}%")

demo_circuit_breaker()
```

## Pattern Scatter-Gather

Diffuser une requ√™te vers plusieurs services, puis rassembler les r√©ponses.

### Scatter-Gather pour recherche distribu√©e

```python
import concurrent.futures
import time
import random
import threading

class SearchService:
    """Service de recherche simul√©."""

    def __init__(self, name, latency_range=(0.1, 1.0), error_rate=0.1):
        self.name = name
        self.latency_range = latency_range
        self.error_rate = error_rate

    def search(self, query):
        """Effectue une recherche."""
        # Simuler latence variable
        time.sleep(random.uniform(*self.latency_range))

        # Simuler des erreurs occasionnelles
        if random.random() < self.error_rate:
            raise Exception(f"{self.name}: Service temporairement indisponible")

        # G√©n√©rer des r√©sultats simul√©s
        results = []
        num_results = random.randint(0, 5)

        for i in range(num_results):
            results.append({
                'title': f"R√©sultat {i+1} pour '{query}'",
                'url': f"https://{self.name.lower()}.com/result_{i+1}",
                'score': random.uniform(0.1, 1.0),
                'source': self.name
            })

        return {
            'service': self.name,
            'query': query,
            'results': results,
            'count': len(results)
        }

class ScatterGather:
    """Impl√©mentation du pattern Scatter-Gather."""

    def __init__(self, services, timeout=3.0):
        self.services = services
        self.timeout = timeout

    def search_all(self, query, min_responses=1):
        """Lance une recherche sur tous les services."""
        print(f"üîç Recherche '{query}' sur {len(self.services)} services")

        start_time = time.time()
        results = []
        errors = []

        # Phase Scatter: lancer toutes les requ√™tes en parall√®le
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.services)) as executor:
            # Soumettre toutes les recherches
            future_to_service = {
                executor.submit(service.search, query): service
                for service in self.services
            }

            # Phase Gather: collecter les r√©ponses
            completed = 0
            for future in concurrent.futures.as_completed(future_to_service, timeout=self.timeout):
                service = future_to_service[future]

                try:
                    result = future.result()
                    results.append(result)
                    completed += 1

                    print(f"‚úÖ {service.name}: {result['count']} r√©sultats "
                          f"({time.time() - start_time:.2f}s)")

                    # Arr√™ter d√®s qu'on a assez de r√©ponses (optionnel)
                    if completed >= min_responses and len(results) >= len(self.services) // 2:
                        break

                except Exception as e:
                    errors.append({
                        'service': service.name,
                        'error': str(e)
                    })
                    print(f"‚ùå {service.name}: {e}")

        total_time = time.time() - start_time

        return {
            'query': query,
            'results': results,
            'errors': errors,
            'total_time': total_time,
            'services_responded': len(results),
            'services_failed': len(errors)
        }

    def aggregate_results(self, search_response):
        """Agr√®ge et trie les r√©sultats de tous les services."""
        all_results = []

        # Collecter tous les r√©sultats
        for service_response in search_response['results']:
            for result in service_response['results']:
                all_results.append(result)

        # Trier par score d√©croissant
        all_results.sort(key=lambda x: x['score'], reverse=True)

        return {
            'query': search_response['query'],
            'total_results': len(all_results),
            'results': all_results[:10],  # Top 10
            'services_used': len(search_response['results']),
            'response_time': search_response['total_time']
        }

def demo_scatter_gather():
    """D√©monstration du pattern Scatter-Gather."""
    print("üì° PATTERN SCATTER-GATHER")
    print("-" * 30)

    # Cr√©er plusieurs services de recherche avec caract√©ristiques diff√©rentes
    services = [
        SearchService("GoogleSearch", latency_range=(0.1, 0.3), error_rate=0.05),
        SearchService("BingSearch", latency_range=(0.2, 0.5), error_rate=0.1),
        SearchService("DuckDuckGo", latency_range=(0.3, 0.8), error_rate=0.15),
        SearchService("YahooSearch", latency_range=(0.4, 1.2), error_rate=0.2),
        SearchService("LocalSearch", latency_range=(0.1, 0.2), error_rate=0.3)
    ]

    scatter_gather = ScatterGather(services, timeout=2.0)

    # Effectuer plusieurs recherches
    queries = ["python concurrence", "machine learning", "web development"]

    for query in queries:
        print(f"\n{'='*50}")

        # Recherche distribu√©e
        search_response = scatter_gather.search_all(query, min_responses=2)

        # Agr√©gation des r√©sultats
        aggregated = scatter_gather.aggregate_results(search_response)

        print(f"\nüìä R√âSULTATS POUR '{query}'")
        print(f"Services r√©pondus: {search_response['services_responded']}/{len(services)}")
        print(f"Temps de r√©ponse: {search_response['total_time']:.2f}s")
        print(f"Total r√©sultats: {aggregated['total_results']}")

        if aggregated['results']:
            print(f"\nTop 3 r√©sultats:")
            for i, result in enumerate(aggregated['results'][:3], 1):
                print(f"  {i}. {result['title']} (score: {result['score']:.2f}) - {result['source']}")

        if search_response['errors']:
            print(f"\nErreurs:")
            for error in search_response['errors']:
                print(f"  ‚ùå {error['service']}: {error['error']}")

        time.sleep(1)  # Pause entre les recherches

demo_scatter_gather()
```

## Exercices pratiques int√©gr√©s

### Exercice 1 : Syst√®me de cache distribu√©

```python
import threading
import time
import random
import hashlib
from collections import defaultdict

class DistributedCache:
    """Cache distribu√© utilisant plusieurs patterns."""

    def __init__(self, num_shards=3, max_size_per_shard=100):
        self.num_shards = num_shards
        self.max_size_per_shard = max_size_per_shard

        # Shards du cache (pattern partitioning)
        self.shards = [
            {'data': {}, 'lock': threading.RLock(), 'access_count': 0}
            for _ in range(num_shards)
        ]

        # Pool de workers pour les op√©rations asynchrones
        self.task_queue = queue.Queue()
        self.workers = []
        self.active = True

        # Statistiques globales
        self.global_stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'evictions': 0
        }
        self.stats_lock = threading.Lock()

    def _hash_key(self, key):
        """Hash une cl√© pour d√©terminer le shard."""
        return int(hashlib.md5(key.encode()).hexdigest(), 16) % self.num_shards

    def _get_shard(self, key):
        """Retourne le shard appropri√© pour une cl√©."""
        shard_id = self._hash_key(key)
        return self.shards[shard_id], shard_id

    def get(self, key):
        """R√©cup√®re une valeur du cache."""
        shard, shard_id = self._get_shard(key)

        with shard['lock']:
            shard['access_count'] += 1

            if key in shard['data']:
                entry = shard['data'][key]

                # V√©rifier l'expiration
                if entry['expires_at'] > time.time():
                    with self.stats_lock:
                        self.global_stats['hits'] += 1

                    print(f"üíæ Cache HIT: {key} (shard {shard_id})")
                    return entry['value']
                else:
                    # Expir√©, supprimer
                    del shard['data'][key]

            with self.stats_lock:
                self.global_stats['misses'] += 1

            print(f"‚ùå Cache MISS: {key} (shard {shard_id})")
            return None

    def set(self, key, value, ttl=300):
        """Stocke une valeur dans le cache."""
        shard, shard_id = self._get_shard(key)

        with shard['lock']:
            # √âviction si n√©cessaire
            if len(shard['data']) >= self.max_size_per_shard and key not in shard['data']:
                self._evict_lru(shard)

            # Stocker la valeur
            shard['data'][key] = {
                'value': value,
                'expires_at': time.time() + ttl,
                'created_at': time.time()
            }

            with self.stats_lock:
                self.global_stats['sets'] += 1

            print(f"üíæ Cache SET: {key} (shard {shard_id}, TTL: {ttl}s)")

    def _evict_lru(self, shard):
        """√âvicte l'√©l√©ment le moins r√©cemment utilis√©."""
        if not shard['data']:
            return

        # Trouver l'entr√©e la plus ancienne
        oldest_key = min(shard['data'].keys(),
                        key=lambda k: shard['data'][k]['created_at'])

        del shard['data'][oldest_key]

        with self.stats_lock:
            self.global_stats['evictions'] += 1

        print(f"üóëÔ∏è √âviction LRU: {oldest_key}")

    def start_background_workers(self, num_workers=2):
        """D√©marre les workers pour les t√¢ches de maintenance."""
        for i in range(num_workers):
            worker = threading.Thread(
                target=self._background_worker,
                args=(f"Worker-{i+1}",)
            )
            worker.start()
            self.workers.append(worker)

        print(f"üîß {num_workers} workers de maintenance d√©marr√©s")

    def _background_worker(self, name):
        """Worker qui nettoie les entr√©es expir√©es."""
        while self.active:
            try:
                # Nettoyer p√©riodiquement
                time.sleep(5)
                self._cleanup_expired()

            except Exception as e:
                print(f"‚ùå {name}: Erreur - {e}")

    def _cleanup_expired(self):
        """Nettoie les entr√©es expir√©es."""
        now = time.time()
        total_cleaned = 0

        for shard_id, shard in enumerate(self.shards):
            with shard['lock']:
                expired_keys = [
                    key for key, entry in shard['data'].items()
                    if entry['expires_at'] <= now
                ]

                for key in expired_keys:
                    del shard['data'][key]
                    total_cleaned += 1

        if total_cleaned > 0:
            print(f"üßπ Nettoyage: {total_cleaned} entr√©es expir√©es supprim√©es")

    def get_stats(self):
        """Retourne les statistiques du cache."""
        with self.stats_lock:
            stats = self.global_stats.copy()

        # Ajouter les stats par shard
        shard_stats = []
        for i, shard in enumerate(self.shards):
            with shard['lock']:
                shard_stats.append({
                    'shard_id': i,
                    'size': len(shard['data']),
                    'access_count': shard['access_count']
                })

        stats['shards'] = shard_stats
        stats['total_size'] = sum(s['size'] for s in shard_stats)

        return stats

    def stop(self):
        """Arr√™te le cache et ses workers."""
        self.active = False

        for worker in self.workers:
            worker.join(timeout=1)

        print("üõë Cache arr√™t√©")

def demo_distributed_cache():
    """Test du cache distribu√©."""
    print("üóÑÔ∏è CACHE DISTRIBU√â")
    print("-" * 20)

    cache = DistributedCache(num_shards=3, max_size_per_shard=5)
    cache.start_background_workers(2)

    def cache_user(name, operations):
        """Simule un utilisateur du cache."""
        for i in range(operations):
            # 70% lecture, 30% √©criture
            if random.random() < 0.7:
                # Lecture
                key = f"key_{random.randint(1, 10)}"
                value = cache.get(key)
            else:
                # √âcriture
                key = f"key_{random.randint(1, 15)}"
                value = f"value_{name}_{i}"
                ttl = random.randint(5, 20)
                cache.set(key, value, ttl)

            time.sleep(random.uniform(0.1, 0.3))

    # Lancer plusieurs utilisateurs
    users = ["User-A", "User-B", "User-C"]
    threads = []

    for user in users:
        t = threading.Thread(target=cache_user, args=(user, 12))
        t.start()
        threads.append(t)

    # Attendre et afficher les stats p√©riodiquement
    for i in range(3):
        time.sleep(4)
        stats = cache.get_stats()

        print(f"\nüìä STATS apr√®s {(i+1)*4}s:")
        print(f"Hits/Misses: {stats['hits']}/{stats['misses']}")
        print(f"Sets/Evictions: {stats['sets']}/{stats['evictions']}")
        print(f"Taille totale: {stats['total_size']}")

        for shard in stats['shards']:
            print(f"  Shard {shard['shard_id']}: {shard['size']} entr√©es, {shard['access_count']} acc√®s")

    # Attendre la fin
    for t in threads:
        t.join()

    cache.stop()

demo_distributed_cache()
```

### Exercice 2 : Orchestrateur de t√¢ches

```python
import threading
import queue
import time
import random
from enum import Enum
from dataclasses import dataclass
from typing import List, Callable, Any

class TaskStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class Task:
    """Repr√©sente une t√¢che dans l'orchestrateur."""
    id: str
    func: Callable
    args: tuple = ()
    kwargs: dict = None
    dependencies: List[str] = None
    priority: int = 0
    timeout: float = 30.0
    retry_count: int = 0
    max_retries: int = 2

    def __post_init__(self):
        if self.kwargs is None:
            self.kwargs = {}
        if self.dependencies is None:
            self.dependencies = []

class TaskOrchestrator:
    """Orchestrateur de t√¢ches avec gestion des d√©pendances."""

    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.tasks = {}  # task_id -> Task
        self.task_status = {}  # task_id -> TaskStatus
        self.task_results = {}  # task_id -> result
        self.ready_queue = queue.PriorityQueue()

        self.workers = []
        self.active = False
        self.lock = threading.RLock()

        # √âv√©nements pour notification
        self.task_completed_event = threading.Event()

    def add_task(self, task: Task):
        """Ajoute une t√¢che √† l'orchestrateur."""
        with self.lock:
            self.tasks[task.id] = task
            self.task_status[task.id] = TaskStatus.PENDING

            print(f"üìã T√¢che ajout√©e: {task.id} (priorit√©: {task.priority})")

            # V√©rifier si la t√¢che peut √™tre ex√©cut√©e imm√©diatement
            self._check_ready_tasks()

    def _check_ready_tasks(self):
        """V√©rifie quelles t√¢ches peuvent √™tre ex√©cut√©es."""
        for task_id, task in self.tasks.items():
            if self.task_status[task_id] == TaskStatus.PENDING:
                if self._dependencies_satisfied(task):
                    # Ajouter √† la queue de priorit√© (n√©gatif pour ordre d√©croissant)
                    self.ready_queue.put((-task.priority, task_id))
                    self.task_status[task_id] = TaskStatus.PENDING
                    print(f"‚úÖ T√¢che pr√™te: {task_id}")

    def _dependencies_satisfied(self, task: Task) -> bool:
        """V√©rifie si toutes les d√©pendances sont satisfaites."""
        for dep_id in task.dependencies:
            if dep_id not in self.task_status:
                return False
            if self.task_status[dep_id] != TaskStatus.COMPLETED:
                return False
        return True

    def start(self):
        """D√©marre l'orchestrateur."""
        if self.active:
            return

        print(f"üöÄ D√©marrage orchestrateur ({self.max_workers} workers)")
        self.active = True

        # D√©marrer les workers
        for i in range(self.max_workers):
            worker = threading.Thread(target=self._worker, args=(f"Worker-{i+1}",))
            worker.start()
            self.workers.append(worker)

        # Thread de monitoring des d√©pendances
        monitor = threading.Thread(target=self._dependency_monitor)
        monitor.start()
        self.workers.append(monitor)

    def _worker(self, name):
        """Worker qui ex√©cute les t√¢ches."""
        print(f"üë∑ {name} d√©marr√©")

        while self.active:
            try:
                # Prendre une t√¢che (timeout pour v√©rifier l'arr√™t)
                try:
                    priority, task_id = self.ready_queue.get(timeout=1)
                except queue.Empty:
                    continue

                # Ex√©cuter la t√¢che
                self._execute_task(task_id, name)
                self.ready_queue.task_done()

            except Exception as e:
                print(f"‚ùå {name}: Erreur - {e}")

        print(f"üõë {name} arr√™t√©")

    def _execute_task(self, task_id, worker_name):
        """Ex√©cute une t√¢che sp√©cifique."""
        with self.lock:
            if self.task_status[task_id] != TaskStatus.PENDING:
                return  # T√¢che d√©j√† prise par un autre worker

            self.task_status[task_id] = TaskStatus.RUNNING
            task = self.tasks[task_id]

        print(f"üîÑ {worker_name}: Ex√©cute {task_id}")

        try:
            # Pr√©parer les arguments avec les r√©sultats des d√©pendances
            kwargs = task.kwargs.copy()

            # Ajouter les r√©sultats des d√©pendances
            for dep_id in task.dependencies:
                if dep_id in self.task_results:
                    kwargs[f"dep_{dep_id}"] = self.task_results[dep_id]

            # Ex√©cuter avec timeout
            start_time = time.time()
            result = task.func(*task.args, **kwargs)
            execution_time = time.time() - start_time

            # Marquer comme termin√©e
            with self.lock:
                self.task_status[task_id] = TaskStatus.COMPLETED
                self.task_results[task_id] = result

            print(f"‚úÖ {worker_name}: {task_id} termin√©e ({execution_time:.2f}s)")

            # Notifier que des t√¢ches peuvent √™tre d√©bloqu√©es
            self.task_completed_event.set()

        except Exception as e:
            print(f"‚ùå {worker_name}: {task_id} √©chou√©e - {e}")

            # Gestion des retry
            task.retry_count += 1
            if task.retry_count <= task.max_retries:
                print(f"üîÑ {task_id}: Retry {task.retry_count}/{task.max_retries}")
                with self.lock:
                    self.task_status[task_id] = TaskStatus.PENDING
                    self.ready_queue.put((-task.priority, task_id))
            else:
                with self.lock:
                    self.task_status[task_id] = TaskStatus.FAILED
                print(f"üí• {task_id}: √âchec d√©finitif apr√®s {task.max_retries} tentatives")

    def _dependency_monitor(self):
        """Monitore les changements pour d√©bloquer de nouvelles t√¢ches."""
        while self.active:
            self.task_completed_event.wait(timeout=1)
            self.task_completed_event.clear()

            # V√©rifier s'il y a de nouvelles t√¢ches √† d√©bloquer
            with self.lock:
                self._check_ready_tasks()

    def wait_all(self, timeout=None):
        """Attend que toutes les t√¢ches se terminent."""
        start_time = time.time()

        while self.active:
            with self.lock:
                pending = sum(1 for status in self.task_status.values()
                            if status in [TaskStatus.PENDING, TaskStatus.RUNNING])

                if pending == 0:
                    break

            if timeout and (time.time() - start_time) > timeout:
                print("‚è∞ Timeout atteint")
                break

            time.sleep(0.5)

        self.stop()

    def stop(self):
        """Arr√™te l'orchestrateur."""
        print("üõë Arr√™t de l'orchestrateur...")
        self.active = False

        for worker in self.workers:
            worker.join(timeout=2)

        self.print_summary()

    def print_summary(self):
        """Affiche un r√©sum√© de l'ex√©cution."""
        with self.lock:
            status_counts = {}
            for status in self.task_status.values():
                status_counts[status] = status_counts.get(status, 0) + 1

        print(f"\nüìä R√âSUM√â DE L'ORCHESTRATION")
        print(f"Total t√¢ches: {len(self.tasks)}")
        for status, count in status_counts.items():
            print(f"{status.value}: {count}")

# Fonctions de test pour l'orchestrateur
def task_a():
    """T√¢che A - Pas de d√©pendances."""
    time.sleep(random.uniform(1, 2))
    return "R√©sultat de A"

def task_b():
    """T√¢che B - Pas de d√©pendances."""
    time.sleep(random.uniform(0.5, 1.5))
    return "R√©sultat de B"

def task_c(dep_task_a, dep_task_b):
    """T√¢che C - D√©pend de A et B."""
    time.sleep(random.uniform(0.8, 1.2))
    return f"R√©sultat de C (utilise {dep_task_a} et {dep_task_b})"

def task_d(dep_task_c):
    """T√¢che D - D√©pend de C."""
    time.sleep(random.uniform(0.3, 0.8))

    # Simuler un √©chec occasionnel
    if random.random() < 0.3:
        raise Exception("√âchec simul√© de la t√¢che D")

    return f"R√©sultat de D (utilise {dep_task_c})"

def demo_task_orchestrator():
    """D√©monstration de l'orchestrateur de t√¢ches."""
    print("üé≠ ORCHESTRATEUR DE T√ÇCHES")
    print("-" * 30)

    orchestrator = TaskOrchestrator(max_workers=2)

    # D√©finir les t√¢ches avec d√©pendances
    tasks = [
        Task("task_a", task_a, priority=1),
        Task("task_b", task_b, priority=1),
        Task("task_c", task_c, dependencies=["task_a", "task_b"], priority=2),
        Task("task_d", task_d, dependencies=["task_c"], priority=3, max_retries=3),
        Task("task_e", lambda: "R√©sultat E", priority=1),  # T√¢che ind√©pendante
    ]

    # D√©marrer l'orchestrateur
    orchestrator.start()

    # Ajouter les t√¢ches
    for task in tasks:
        orchestrator.add_task(task)
        time.sleep(0.2)  # Petit d√©lai pour voir l'ordre

    # Attendre la fin
    orchestrator.wait_all(timeout=30)

demo_task_orchestrator()
```

## R√©capitulatif des patterns

### Tableau de comparaison

| Pattern | Usage principal | Avantages | Inconv√©nients |
|---------|-----------------|-----------|---------------|
| **Producer-Consumer** | Communication async | D√©couplage, gestion de charge | Complexit√© de synchronisation |
| **Worker Pool** | Traitement parall√®le | Utilisation optimale des ressources | Pas de d√©pendances entre t√¢ches |
| **Pipeline** | Traitement s√©quentiel | D√©bit √©lev√©, sp√©cialisation | S√©quentiel, point de bottleneck |
| **Map-Reduce** | Gros volumes de donn√©es | Scalabilit√© horizontale | Overhead de coordination |
| **Circuit Breaker** | R√©silience des services | Protection contre les cascades | Faux positifs possibles |
| **Scatter-Gather** | Recherche distribu√©e | Latence r√©duite, redondance | Complexit√© de gestion |

### Conseils de choix

```python
# ‚úÖ Producer-Consumer : Communication asynchrone
if "communication" in requirements and "d√©couplage" in requirements:
    use_producer_consumer()

# ‚úÖ Worker Pool : T√¢ches ind√©pendantes parall√®les
if "parall√©lisme" in requirements and "t√¢ches_ind√©pendantes" in requirements:
    use_worker_pool()

# ‚úÖ Pipeline : Traitement en √©tapes
if "√©tapes_s√©quentielles" in requirements and "d√©bit" in requirements:
    use_pipeline()

# ‚úÖ Map-Reduce : Gros volumes de donn√©es
if "big_data" in requirements and "calcul_distribu√©" in requirements:
    use_map_reduce()
```

## Bonnes pratiques g√©n√©rales

### **1. Choisir le bon pattern**
```python
# Analyser le probl√®me avant de choisir
- Type de donn√©es (flux vs batch)
- D√©pendances entre t√¢ches
- Contraintes de performance
- Niveau de parall√©lisme souhait√©
```

### **2. Monitoring et observabilit√©**
```python
# Toujours inclure des m√©triques
class PatternWithMetrics:
    def __init__(self):
        self.metrics = {
            'processed': 0,
            'errors': 0,
            'avg_time': 0
        }

    def track_operation(self, duration, success):
        self.metrics['processed'] += 1
        if not success:
            self.metrics['errors'] += 1
```

### **3. Gestion d'erreurs robuste**
```python
# G√©rer les pannes gracieusement
try:
    result = process_task(task)
except RecoverableError:
    retry_task(task)
except FatalError:
    mark_failed(task)
    notify_admin()
```

### **4. Timeouts et annulation**
```python
# Toujours inclure des timeouts
import signal
from contextlib import contextmanager

@contextmanager
def timeout(seconds):
    def timeout_handler(signum, frame):
        raise TimeoutError(f"Op√©ration d√©pass√©e ({seconds}s)")

    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(seconds)

    try:
        yield
    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)

# Usage
try:
    with timeout(30):
        result = long_running_operation()
except TimeoutError:
    print("Op√©ration annul√©e (timeout)")
```

### **5. Backpressure et contr√¥le de flux**
```python
# √âviter la surcharge avec backpressure
class BackpressureQueue:
    def __init__(self, maxsize, backpressure_threshold=0.8):
        self.queue = queue.Queue(maxsize)
        self.threshold = int(maxsize * backpressure_threshold)

    def put(self, item, timeout=None):
        if self.queue.qsize() > self.threshold:
            print("‚ö†Ô∏è Backpressure activ√©e - ralentissement")
            time.sleep(0.1)  # Ralentir le producteur

        return self.queue.put(item, timeout=timeout)
```

## Projet final : Syst√®me de traitement de donn√©es distribu√©

Cr√©ons un syst√®me complet qui combine tous les patterns appris.

### Architecture du syst√®me

```python
import threading
import queue
import time
import random
import json
import logging
from datetime import datetime
from enum import Enum
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Callable
import concurrent.futures

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class JobType(Enum):
    DATA_INGESTION = "data_ingestion"
    DATA_PROCESSING = "data_processing"
    DATA_ANALYSIS = "data_analysis"
    REPORT_GENERATION = "report_generation"

class JobStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class Job:
    """Repr√©sente un job de traitement."""
    id: str
    type: JobType
    data: Dict[str, Any]
    priority: int = 0
    dependencies: List[str] = None
    timeout: float = 60.0
    max_retries: int = 2
    created_at: float = None

    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []
        if self.created_at is None:
            self.created_at = time.time()

class DistributedDataProcessor:
    """Syst√®me de traitement de donn√©es distribu√©."""

    def __init__(self,
                 ingestion_workers=2,
                 processing_workers=3,
                 analysis_workers=2,
                 report_workers=1):

        self.logger = logging.getLogger(self.__class__.__name__)

        # Configuration des workers par type
        self.worker_config = {
            JobType.DATA_INGESTION: ingestion_workers,
            JobType.DATA_PROCESSING: processing_workers,
            JobType.DATA_ANALYSIS: analysis_workers,
            JobType.REPORT_GENERATION: report_workers
        }

        # Queues sp√©cialis√©es pour chaque type de job
        self.job_queues = {
            job_type: queue.PriorityQueue()
            for job_type in JobType
        }

        # √âtat global du syst√®me
        self.jobs = {}  # job_id -> Job
        self.job_status = {}  # job_id -> JobStatus
        self.job_results = {}  # job_id -> result
        self.job_errors = {}  # job_id -> error_info

        # Workers par type
        self.workers = {job_type: [] for job_type in JobType}
        self.active = False

        # Synchronisation
        self.lock = threading.RLock()
        self.job_completed_event = threading.Event()

        # Statistiques
        self.stats = {
            'jobs_submitted': 0,
            'jobs_completed': 0,
            'jobs_failed': 0,
            'total_processing_time': 0
        }

        # Circuit breakers pour chaque type de job
        self.circuit_breakers = {
            job_type: CircuitBreaker(failure_threshold=3, timeout=30)
            for job_type in JobType
        }

    def submit_job(self, job: Job) -> bool:
        """Soumet un job pour traitement."""
        with self.lock:
            # V√©rifier les d√©pendances
            for dep_id in job.dependencies:
                if dep_id not in self.jobs:
                    self.logger.error(f"D√©pendance manquante pour {job.id}: {dep_id}")
                    return False

            # Enregistrer le job
            self.jobs[job.id] = job
            self.job_status[job.id] = JobStatus.PENDING
            self.stats['jobs_submitted'] += 1

            self.logger.info(f"Job soumis: {job.id} ({job.type.value})")

            # V√©rifier si le job peut √™tre ex√©cut√© imm√©diatement
            self._check_ready_jobs()

            return True

    def _check_ready_jobs(self):
        """V√©rifie quels jobs peuvent √™tre ex√©cut√©s."""
        for job_id, job in self.jobs.items():
            if self.job_status[job_id] == JobStatus.PENDING:
                if self._dependencies_satisfied(job):
                    # Ajouter √† la queue appropri√©e
                    priority_item = (-job.priority, time.time(), job_id)
                    self.job_queues[job.type].put(priority_item)
                    self.logger.info(f"Job pr√™t: {job_id}")

    def _dependencies_satisfied(self, job: Job) -> bool:
        """V√©rifie si toutes les d√©pendances sont satisfaites."""
        for dep_id in job.dependencies:
            if (dep_id not in self.job_status or
                self.job_status[dep_id] != JobStatus.COMPLETED):
                return False
        return True

    def start(self):
        """D√©marre le syst√®me de traitement."""
        if self.active:
            self.logger.warning("Syst√®me d√©j√† d√©marr√©")
            return

        self.logger.info("üöÄ D√©marrage du syst√®me de traitement distribu√©")
        self.active = True

        # D√©marrer les workers pour chaque type de job
        for job_type, worker_count in self.worker_config.items():
            for i in range(worker_count):
                worker_name = f"{job_type.value}-worker-{i+1}"
                worker = threading.Thread(
                    target=self._worker,
                    args=(worker_name, job_type)
                )
                worker.start()
                self.workers[job_type].append(worker)

        # D√©marrer le moniteur de d√©pendances
        monitor = threading.Thread(target=self._dependency_monitor)
        monitor.start()

        # D√©marrer le collecteur de m√©triques
        metrics_collector = threading.Thread(target=self._metrics_collector)
        metrics_collector.start()

        self.logger.info("‚úÖ Syst√®me d√©marr√©")

    def _worker(self, worker_name: str, job_type: JobType):
        """Worker sp√©cialis√© pour un type de job."""
        logger = logging.getLogger(f"Worker.{worker_name}")
        logger.info(f"Worker {worker_name} d√©marr√©")

        job_queue = self.job_queues[job_type]

        while self.active:
            try:
                # Prendre un job avec timeout
                try:
                    priority, submit_time, job_id = job_queue.get(timeout=1)
                except queue.Empty:
                    continue

                # V√©rifier que le job est toujours valide
                with self.lock:
                    if (job_id not in self.job_status or
                        self.job_status[job_id] != JobStatus.PENDING):
                        job_queue.task_done()
                        continue

                    self.job_status[job_id] = JobStatus.RUNNING
                    job = self.jobs[job_id]

                # Ex√©cuter le job avec circuit breaker
                try:
                    circuit_breaker = self.circuit_breakers[job_type]
                    result = circuit_breaker.call(
                        self._execute_job_by_type,
                        job, worker_name
                    )

                    # Job r√©ussi
                    with self.lock:
                        self.job_status[job_id] = JobStatus.COMPLETED
                        self.job_results[job_id] = result
                        self.stats['jobs_completed'] += 1

                    logger.info(f"‚úÖ Job {job_id} termin√© avec succ√®s")
                    self.job_completed_event.set()

                except Exception as e:
                    # Job √©chou√©
                    logger.error(f"‚ùå Job {job_id} √©chou√©: {e}")

                    with self.lock:
                        if job_id not in self.job_errors:
                            self.job_errors[job_id] = []

                        self.job_errors[job_id].append({
                            'error': str(e),
                            'timestamp': time.time(),
                            'worker': worker_name
                        })

                        # Gestion des retry
                        retry_count = len(self.job_errors[job_id])
                        if retry_count <= job.max_retries:
                            logger.info(f"üîÑ Retry {retry_count}/{job.max_retries} pour {job_id}")
                            self.job_status[job_id] = JobStatus.PENDING
                            # Remettre en queue avec priorit√© r√©duite
                            retry_priority = (-max(job.priority - retry_count, 0), time.time(), job_id)
                            job_queue.put(retry_priority)
                        else:
                            self.job_status[job_id] = JobStatus.FAILED
                            self.stats['jobs_failed'] += 1
                            logger.error(f"üí• Job {job_id} √©chec d√©finitif")

                job_queue.task_done()

            except Exception as e:
                logger.error(f"Erreur worker {worker_name}: {e}")

        logger.info(f"Worker {worker_name} arr√™t√©")

    def _execute_job_by_type(self, job: Job, worker_name: str) -> Any:
        """Ex√©cute un job selon son type."""
        start_time = time.time()

        # Pr√©parer les donn√©es avec les r√©sultats des d√©pendances
        job_data = job.data.copy()
        for dep_id in job.dependencies:
            if dep_id in self.job_results:
                job_data[f'dep_{dep_id}'] = self.job_results[dep_id]

        # Ex√©cuter selon le type
        if job.type == JobType.DATA_INGESTION:
            result = self._process_ingestion(job_data)
        elif job.type == JobType.DATA_PROCESSING:
            result = self._process_data_processing(job_data)
        elif job.type == JobType.DATA_ANALYSIS:
            result = self._process_analysis(job_data)
        elif job.type == JobType.REPORT_GENERATION:
            result = self._process_report_generation(job_data)
        else:
            raise ValueError(f"Type de job non support√©: {job.type}")

        # Enregistrer le temps de traitement
        processing_time = time.time() - start_time
        with self.lock:
            self.stats['total_processing_time'] += processing_time

        return result

    def _process_ingestion(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Simule l'ingestion de donn√©es."""
        time.sleep(random.uniform(0.5, 2.0))  # Simuler I/O

        # Simuler une erreur occasionnelle
        if random.random() < 0.1:
            raise Exception("Erreur d'ingestion de donn√©es")

        return {
            'type': 'ingestion_result',
            'records_ingested': random.randint(100, 1000),
            'source': data.get('source', 'unknown'),
            'timestamp': time.time()
        }

    def _process_data_processing(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Simule le traitement de donn√©es."""
        time.sleep(random.uniform(1.0, 3.0))  # Simuler calculs

        # Simuler une erreur occasionnelle
        if random.random() < 0.15:
            raise Exception("Erreur de traitement")

        # Utiliser les donn√©es d'ingestion si disponibles
        base_records = 500
        for key, value in data.items():
            if key.startswith('dep_') and isinstance(value, dict):
                base_records = value.get('records_ingested', base_records)
                break

        return {
            'type': 'processing_result',
            'records_processed': base_records,
            'records_valid': int(base_records * random.uniform(0.8, 0.95)),
            'processing_time': time.time()
        }

    def _process_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Simule l'analyse de donn√©es."""
        time.sleep(random.uniform(2.0, 4.0))  # Simuler analyse complexe

        # Simuler une erreur occasionnelle
        if random.random() < 0.05:
            raise Exception("Erreur d'analyse")

        # Utiliser les donn√©es trait√©es si disponibles
        base_records = 400
        for key, value in data.items():
            if key.startswith('dep_') and isinstance(value, dict):
                base_records = value.get('records_valid', base_records)
                break

        return {
            'type': 'analysis_result',
            'insights_generated': random.randint(5, 15),
            'anomalies_detected': random.randint(0, 3),
            'confidence_score': random.uniform(0.7, 0.95),
            'analyzed_records': base_records
        }

    def _process_report_generation(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Simule la g√©n√©ration de rapport."""
        time.sleep(random.uniform(1.0, 2.0))  # Simuler g√©n√©ration

        # Agr√©ger les donn√©es des d√©pendances
        total_records = 0
        total_insights = 0

        for key, value in data.items():
            if key.startswith('dep_') and isinstance(value, dict):
                total_records += value.get('analyzed_records', 0)
                total_insights += value.get('insights_generated', 0)

        return {
            'type': 'report_result',
            'report_id': f"report_{int(time.time())}",
            'total_records_analyzed': total_records,
            'total_insights': total_insights,
            'report_size_mb': random.uniform(1.0, 10.0),
            'generated_at': time.time()
        }

    def _dependency_monitor(self):
        """Monitore les changements pour d√©bloquer de nouveaux jobs."""
        while self.active:
            self.job_completed_event.wait(timeout=2)
            self.job_completed_event.clear()

            with self.lock:
                self._check_ready_jobs()

    def _metrics_collector(self):
        """Collecte les m√©triques du syst√®me."""
        while self.active:
            time.sleep(10)  # Collecter toutes les 10 secondes

            with self.lock:
                stats = self.get_stats()

            self.logger.info(f"üìä M√©triques: {stats['jobs_completed']} compl√©t√©s, "
                           f"{stats['jobs_failed']} √©chou√©s, "
                           f"{stats['pending_jobs']} en attente")

    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du syst√®me."""
        with self.lock:
            stats = self.stats.copy()

            # Compter les jobs par statut
            status_counts = {}
            for status in self.job_status.values():
                status_counts[status.value] = status_counts.get(status.value, 0) + 1

            stats.update(status_counts)
            stats['pending_jobs'] = status_counts.get('pending', 0)
            stats['running_jobs'] = status_counts.get('running', 0)

            # Calculer les temps moyens
            if stats['jobs_completed'] > 0:
                stats['avg_processing_time'] = (
                    stats['total_processing_time'] / stats['jobs_completed']
                )
            else:
                stats['avg_processing_time'] = 0

            # √âtat des queues
            queue_sizes = {}
            for job_type, job_queue in self.job_queues.items():
                queue_sizes[f'{job_type.value}_queue_size'] = job_queue.qsize()

            stats.update(queue_sizes)

            return stats

    def wait_completion(self, timeout: float = None) -> bool:
        """Attend que tous les jobs se terminent."""
        start_time = time.time()

        while self.active:
            with self.lock:
                pending = sum(1 for status in self.job_status.values()
                            if status in [JobStatus.PENDING, JobStatus.RUNNING])

                if pending == 0:
                    self.logger.info("‚úÖ Tous les jobs termin√©s")
                    return True

            if timeout and (time.time() - start_time) > timeout:
                self.logger.warning("‚è∞ Timeout atteint")
                return False

            time.sleep(1)

        return True

    def stop(self, timeout: float = 30):
        """Arr√™te le syst√®me."""
        self.logger.info("üõë Arr√™t du syst√®me...")
        self.active = False

        # Attendre que tous les workers se terminent
        all_workers = []
        for workers in self.workers.values():
            all_workers.extend(workers)

        for worker in all_workers:
            worker.join(timeout=timeout/len(all_workers))

        self.print_final_stats()

    def print_final_stats(self):
        """Affiche les statistiques finales."""
        stats = self.get_stats()

        print(f"\n{'='*50}")
        print("üìä STATISTIQUES FINALES DU SYST√àME")
        print(f"{'='*50}")
        print(f"Jobs soumis: {stats['jobs_submitted']}")
        print(f"Jobs compl√©t√©s: {stats['jobs_completed']}")
        print(f"Jobs √©chou√©s: {stats['jobs_failed']}")
        print(f"Temps de traitement moyen: {stats['avg_processing_time']:.2f}s")

        success_rate = 0
        if stats['jobs_submitted'] > 0:
            success_rate = (stats['jobs_completed'] / stats['jobs_submitted']) * 100
        print(f"Taux de succ√®s: {success_rate:.1f}%")

        # D√©tail par statut
        print(f"\nR√©partition des jobs:")
        for status in JobStatus:
            count = stats.get(status.value, 0)
            if count > 0:
                print(f"  {status.value}: {count}")

def demo_distributed_system():
    """D√©monstration du syst√®me distribu√© complet."""
    print("üåê SYST√àME DE TRAITEMENT DISTRIBU√â")
    print("="*50)

    # Cr√©er le syst√®me
    system = DistributedDataProcessor(
        ingestion_workers=2,
        processing_workers=2,
        analysis_workers=2,
        report_workers=1
    )

    try:
        # D√©marrer le syst√®me
        system.start()
        time.sleep(1)  # Laisser le syst√®me se stabiliser

        # Cr√©er un pipeline de jobs interd√©pendants
        jobs = [
            # Phase 1: Ingestion de donn√©es (parall√®le)
            Job("ingest_sales", JobType.DATA_INGESTION,
                {"source": "sales_db"}, priority=3),
            Job("ingest_customers", JobType.DATA_INGESTION,
                {"source": "customer_db"}, priority=3),
            Job("ingest_products", JobType.DATA_INGESTION,
                {"source": "product_db"}, priority=2),

            # Phase 2: Traitement (d√©pend de l'ingestion)
            Job("process_sales", JobType.DATA_PROCESSING,
                {"operation": "clean_sales"},
                dependencies=["ingest_sales"], priority=2),
            Job("process_customers", JobType.DATA_PROCESSING,
                {"operation": "clean_customers"},
                dependencies=["ingest_customers"], priority=2),

            # Phase 3: Analyse (d√©pend du traitement)
            Job("analyze_trends", JobType.DATA_ANALYSIS,
                {"analysis_type": "trend_analysis"},
                dependencies=["process_sales", "process_customers"], priority=1),
            Job("analyze_segments", JobType.DATA_ANALYSIS,
                {"analysis_type": "customer_segmentation"},
                dependencies=["process_customers"], priority=1),

            # Phase 4: G√©n√©ration de rapports (d√©pend de l'analyse)
            Job("generate_monthly_report", JobType.REPORT_GENERATION,
                {"report_type": "monthly_summary"},
                dependencies=["analyze_trends", "analyze_segments"], priority=1),
        ]

        # Soumettre tous les jobs
        print(f"\nüìã Soumission de {len(jobs)} jobs...")
        for job in jobs:
            success = system.submit_job(job)
            if success:
                print(f"‚úÖ Job soumis: {job.id}")
            else:
                print(f"‚ùå √âchec soumission: {job.id}")
            time.sleep(0.2)

        # Surveiller le progr√®s
        print(f"\n‚è≥ Traitement en cours...")

        # Attendre la completion avec timeout
        completed = system.wait_completion(timeout=60)

        if not completed:
            print("‚è∞ Certains jobs n'ont pas termin√© dans les temps")

    finally:
        # Arr√™ter le syst√®me
        system.stop()

if __name__ == "__main__":
    demo_distributed_system()
```

## Conclusion du Module 8

### R√©capitulatif des patterns appris

1. **Producer-Consumer** : Communication asynchrone d√©coupl√©e
2. **Worker Pool** : Parall√©lisation de t√¢ches ind√©pendantes
3. **Pipeline** : Traitement en √©tapes s√©quentielles
4. **Map-Reduce** : Traitement distribu√© de gros volumes
5. **Circuit Breaker** : Protection contre les d√©faillances
6. **Scatter-Gather** : Requ√™tes distribu√©es avec agr√©gation

### Comp√©tences acquises

- ‚úÖ **Analyser un probl√®me** de concurrence
- ‚úÖ **Choisir le pattern appropri√©** selon le contexte
- ‚úÖ **Impl√©menter des solutions robustes** avec gestion d'erreurs
- ‚úÖ **G√©rer la synchronisation** et √©viter les race conditions
- ‚úÖ **Monitorer et d√©boguer** du code concurrent
- ‚úÖ **Combiner plusieurs patterns** dans un syst√®me complexe

### Projet final : Points cl√©s

Le syst√®me distribu√© que nous avons construit d√©montre :

- **Architecture modulaire** avec workers sp√©cialis√©s
- **Gestion des d√©pendances** entre jobs
- **Circuit breakers** pour la r√©silience
- **Monitoring et m√©triques** en temps r√©el
- **Retry et recovery** automatiques
- **Scalabilit√©** par type de t√¢che

### Conseils pour vos projets

1. **Commencez simple** : Un seul pattern √† la fois
2. **Mesurez d'abord** : Profilez avant d'optimiser
3. **G√©rez les erreurs** : La concurrence amplifie les bugs
4. **Testez sous charge** : Les bugs de concurrence sont sournois
5. **Documentez l'architecture** : Essentiel pour la maintenance

La programmation concurrente est un domaine complexe mais puissant. Avec les patterns et techniques que nous avons explor√©s, vous √™tes maintenant √©quip√©s pour cr√©er des applications performantes et scalables !

üéâ **F√©licitations pour avoir termin√© le Module 8 sur la programmation concurrente !**

‚è≠Ô∏è
