ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 8.4 : Patterns de concurrence

## Introduction

Les patterns de concurrence sont des solutions Ã©prouvÃ©es pour rÃ©soudre des problÃ¨mes rÃ©currents en programmation parallÃ¨le. Ces modÃ¨les vous aident Ã  structurer vos applications concurrentes de maniÃ¨re efficace et maintenable.

### Analogie simple
Imaginez les patterns comme des **recettes de cuisine Ã©prouvÃ©es** :
- **Producer-Consumer** : Une chaÃ®ne de production (fabricant â†’ convoyeur â†’ emballeur)
- **Worker Pool** : Une Ã©quipe de cuisiniers qui se partagent les commandes
- **Pipeline** : Une chaÃ®ne de montage oÃ¹ chaque station a une fonction
- **Map-Reduce** : Diviser une grosse tÃ¢che, traiter en parallÃ¨le, puis combiner

## Pattern Producer-Consumer

Le pattern le plus fondamental : des producteurs crÃ©ent des donnÃ©es, des consommateurs les traitent.

### Implementation basique

```python
import threading
import queue
import time
import random

class ProducerConsumer:
    """Implementation du pattern Producer-Consumer."""

    def __init__(self, queue_size=10):
        self.queue = queue.Queue(maxsize=queue_size)
        self.active = True
        self.stats = {
            'produced': 0,
            'consumed': 0,
            'errors': 0
        }
        self.stats_lock = threading.Lock()

    def producer(self, name, items_count, delay_range=(0.1, 0.5)):
        """Produit des Ã©lÃ©ments Ã  intervalles rÃ©guliers."""
        print(f"ğŸ­ {name} dÃ©marrÃ© (production de {items_count} Ã©lÃ©ments)")

        for i in range(items_count):
            if not self.active:
                break

            # CrÃ©er un Ã©lÃ©ment
            item = {
                'id': f"{name}_{i+1}",
                'data': random.randint(1, 100),
                'timestamp': time.time(),
                'producer': name
            }

            try:
                # Ajouter Ã  la queue (bloque si pleine)
                self.queue.put(item, timeout=2)

                with self.stats_lock:
                    self.stats['produced'] += 1

                print(f"ğŸ“¦ {name}: Produit {item['id']} (data={item['data']})")

                # DÃ©lai variable
                time.sleep(random.uniform(*delay_range))

            except queue.Full:
                print(f"âš ï¸ {name}: Queue pleine, Ã©lÃ©ment perdu")
                with self.stats_lock:
                    self.stats['errors'] += 1

        print(f"âœ… {name}: Production terminÃ©e")

    def consumer(self, name, processing_time_range=(0.2, 0.8)):
        """Consomme et traite des Ã©lÃ©ments."""
        print(f"ğŸ”„ {name} dÃ©marrÃ©")

        while self.active:
            try:
                # RÃ©cupÃ©rer un Ã©lÃ©ment (bloque si vide)
                item = self.queue.get(timeout=1)

                print(f"ğŸ“¥ {name}: Traite {item['id']}")

                # Simuler le traitement
                processing_time = random.uniform(*processing_time_range)
                time.sleep(processing_time)

                # Marquer comme terminÃ©
                self.queue.task_done()

                with self.stats_lock:
                    self.stats['consumed'] += 1

                print(f"âœ… {name}: TerminÃ© {item['id']} ({processing_time:.2f}s)")

            except queue.Empty:
                # Timeout, vÃ©rifier si on doit continuer
                continue
            except Exception as e:
                print(f"âŒ {name}: Erreur - {e}")
                with self.stats_lock:
                    self.stats['errors'] += 1

        print(f"ğŸ›‘ {name}: ArrÃªtÃ©")

    def run(self, producers_config, consumers_config, duration=10):
        """Lance le systÃ¨me producer-consumer."""
        print("ğŸš€ DÃ©marrage du systÃ¨me Producer-Consumer")
        print(f"Producteurs: {len(producers_config)}, Consommateurs: {len(consumers_config)}")
        print(f"DurÃ©e: {duration}s\n")

        threads = []

        # DÃ©marrer les producteurs
        for name, items_count in producers_config:
            t = threading.Thread(target=self.producer, args=(name, items_count))
            t.start()
            threads.append(t)

        # DÃ©marrer les consommateurs
        for name in consumers_config:
            t = threading.Thread(target=self.consumer, args=(name,))
            t.start()
            threads.append(t)

        # Laisser fonctionner
        time.sleep(duration)

        # ArrÃªter le systÃ¨me
        print("\nğŸ›‘ ArrÃªt du systÃ¨me...")
        self.active = False

        # Attendre les producteurs
        for t in threads[:len(producers_config)]:
            t.join()

        # Vider la queue restante
        try:
            while True:
                self.queue.get_nowait()
                self.queue.task_done()
        except queue.Empty:
            pass

        # Attendre les consommateurs
        for t in threads[len(producers_config):]:
            t.join(timeout=2)

        self.print_stats()

    def print_stats(self):
        """Affiche les statistiques finales."""
        with self.stats_lock:
            stats = self.stats.copy()

        print(f"\nğŸ“Š STATISTIQUES FINALES")
        print(f"Produits: {stats['produced']}")
        print(f"ConsommÃ©s: {stats['consumed']}")
        print(f"Erreurs: {stats['errors']}")
        print(f"Queue restante: {self.queue.qsize()}")

# Test du pattern
def demo_producer_consumer():
    """DÃ©monstration du pattern Producer-Consumer."""
    pc = ProducerConsumer(queue_size=5)

    producers = [("Prod-A", 8), ("Prod-B", 6)]
    consumers = ["Cons-1", "Cons-2", "Cons-3"]

    pc.run(producers, consumers, duration=12)

demo_producer_consumer()
```

## Pattern Worker Pool

Pool de workers qui se partagent les tÃ¢ches d'une queue commune.

### Worker Pool avec ThreadPoolExecutor

```python
import concurrent.futures
import time
import random

def tache_worker(task_data):
    """Fonction exÃ©cutÃ©e par un worker."""
    task_id = task_data['id']
    complexity = task_data['complexity']

    print(f"ğŸ”„ Worker traite tÃ¢che {task_id} (complexitÃ©: {complexity})")

    # Simuler le travail (plus c'est complexe, plus c'est long)
    work_time = complexity * random.uniform(0.1, 0.3)
    time.sleep(work_time)

    # Simuler des erreurs occasionnelles
    if random.random() < 0.1:
        raise Exception(f"Erreur simulÃ©e pour tÃ¢che {task_id}")

    result = {
        'task_id': task_id,
        'result': f"RÃ©sultat_{task_id}",
        'work_time': work_time,
        'worker_thread': threading.current_thread().name
    }

    print(f"âœ… TÃ¢che {task_id} terminÃ©e ({work_time:.2f}s)")
    return result

class WorkerPool:
    """Gestionnaire de pool de workers."""

    def __init__(self, max_workers=4):
        self.max_workers = max_workers

    def process_tasks(self, tasks):
        """Traite une liste de tÃ¢ches avec le pool."""
        print(f"ğŸŠ Pool de {self.max_workers} workers pour {len(tasks)} tÃ¢ches")

        results = []
        errors = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Soumettre toutes les tÃ¢ches
            future_to_task = {
                executor.submit(tache_worker, task): task
                for task in tasks
            }

            # RÃ©cupÃ©rer les rÃ©sultats au fur et Ã  mesure
            for future in concurrent.futures.as_completed(future_to_task):
                task = future_to_task[future]

                try:
                    result = future.result()
                    results.append(result)

                except Exception as e:
                    error_info = {
                        'task_id': task['id'],
                        'error': str(e)
                    }
                    errors.append(error_info)
                    print(f"âŒ Erreur tÃ¢che {task['id']}: {e}")

        return results, errors

    def process_with_timeout(self, tasks, timeout_per_task=5):
        """Traite avec timeout par tÃ¢che."""
        print(f"â° Pool avec timeout de {timeout_per_task}s par tÃ¢che")

        results = []
        errors = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Soumettre toutes les tÃ¢ches
            futures = [executor.submit(tache_worker, task) for task in tasks]

            # Attendre avec timeout
            for i, future in enumerate(futures):
                try:
                    result = future.result(timeout=timeout_per_task)
                    results.append(result)

                except concurrent.futures.TimeoutError:
                    future.cancel()
                    error_info = {
                        'task_id': tasks[i]['id'],
                        'error': 'Timeout'
                    }
                    errors.append(error_info)
                    print(f"â° Timeout tÃ¢che {tasks[i]['id']}")

                except Exception as e:
                    error_info = {
                        'task_id': tasks[i]['id'],
                        'error': str(e)
                    }
                    errors.append(error_info)

        return results, errors

def demo_worker_pool():
    """DÃ©monstration du pattern Worker Pool."""
    print("ğŸ‘¥ PATTERN WORKER POOL")
    print("-" * 25)

    # CrÃ©er des tÃ¢ches variÃ©es
    tasks = [
        {'id': f'task_{i}', 'complexity': random.randint(1, 5)}
        for i in range(1, 13)
    ]

    pool = WorkerPool(max_workers=3)

    start_time = time.time()
    results, errors = pool.process_tasks(tasks)
    total_time = time.time() - start_time

    print(f"\nğŸ“Š RÃ‰SULTATS")
    print(f"TÃ¢ches rÃ©ussies: {len(results)}")
    print(f"Erreurs: {len(errors)}")
    print(f"Temps total: {total_time:.2f}s")

    # Afficher qui a fait quoi
    worker_stats = {}
    for result in results:
        worker = result['worker_thread']
        worker_stats[worker] = worker_stats.get(worker, 0) + 1

    print(f"\nRÃ©partition par worker:")
    for worker, count in worker_stats.items():
        print(f"  {worker}: {count} tÃ¢ches")

demo_worker_pool()
```

## Pattern Pipeline

Traitement en pipeline oÃ¹ chaque Ã©tape transforme les donnÃ©es.

### Pipeline de traitement d'images

```python
import threading
import queue
import time
import random

class PipelineStage:
    """Ã‰tape gÃ©nÃ©rique d'un pipeline."""

    def __init__(self, name, process_func, input_queue=None, output_queue=None):
        self.name = name
        self.process_func = process_func
        self.input_queue = input_queue or queue.Queue()
        self.output_queue = output_queue
        self.active = True
        self.stats = {'processed': 0, 'errors': 0}

    def run(self):
        """ExÃ©cute l'Ã©tape du pipeline."""
        print(f"ğŸ”§ {self.name} dÃ©marrÃ©")

        while self.active:
            try:
                # RÃ©cupÃ©rer un Ã©lÃ©ment Ã  traiter
                item = self.input_queue.get(timeout=1)

                if item is None:  # Signal d'arrÃªt
                    break

                # Traiter l'Ã©lÃ©ment
                print(f"ğŸ”„ {self.name}: Traite {item.get('id', 'item')}")

                try:
                    result = self.process_func(item)

                    # Envoyer Ã  l'Ã©tape suivante
                    if self.output_queue:
                        self.output_queue.put(result)

                    self.stats['processed'] += 1
                    print(f"âœ… {self.name}: TerminÃ© {item.get('id', 'item')}")

                except Exception as e:
                    print(f"âŒ {self.name}: Erreur - {e}")
                    self.stats['errors'] += 1

                self.input_queue.task_done()

            except queue.Empty:
                continue

        print(f"ğŸ›‘ {self.name} arrÃªtÃ©")

class ImagePipeline:
    """Pipeline de traitement d'images."""

    def __init__(self):
        # Queues entre les Ã©tapes
        self.queue_load = queue.Queue()
        self.queue_resize = queue.Queue()
        self.queue_filter = queue.Queue()
        self.queue_save = queue.Queue()

        # Ã‰tapes du pipeline
        self.stages = [
            PipelineStage("Loader", self.load_image, self.queue_load, self.queue_resize),
            PipelineStage("Resizer", self.resize_image, self.queue_resize, self.queue_filter),
            PipelineStage("Filter", self.apply_filter, self.queue_filter, self.queue_save),
            PipelineStage("Saver", self.save_image, self.queue_save, None)
        ]

        self.threads = []

    def load_image(self, request):
        """Ã‰tape 1: Charger l'image."""
        time.sleep(random.uniform(0.1, 0.3))  # Simuler I/O

        return {
            'id': request['id'],
            'filename': request['filename'],
            'data': f"raw_data_{request['id']}",
            'stage': 'loaded'
        }

    def resize_image(self, image):
        """Ã‰tape 2: Redimensionner."""
        time.sleep(random.uniform(0.2, 0.4))  # Simuler traitement

        image['data'] = f"resized_{image['data']}"
        image['stage'] = 'resized'
        return image

    def apply_filter(self, image):
        """Ã‰tape 3: Appliquer un filtre."""
        time.sleep(random.uniform(0.1, 0.2))  # Simuler traitement

        image['data'] = f"filtered_{image['data']}"
        image['stage'] = 'filtered'
        return image

    def save_image(self, image):
        """Ã‰tape 4: Sauvegarder."""
        time.sleep(random.uniform(0.1, 0.3))  # Simuler I/O

        image['data'] = f"saved_{image['data']}"
        image['stage'] = 'saved'
        return image

    def start(self):
        """DÃ©marre le pipeline."""
        print("ğŸš€ DÃ©marrage du pipeline de traitement")

        # DÃ©marrer chaque Ã©tape dans son propre thread
        for stage in self.stages:
            thread = threading.Thread(target=stage.run)
            thread.start()
            self.threads.append(thread)

        print("âœ… Pipeline dÃ©marrÃ©")

    def process_images(self, image_requests):
        """Traite une liste de demandes d'images."""
        print(f"ğŸ“¸ Traitement de {len(image_requests)} images")

        # Injecter les demandes dans le pipeline
        for request in image_requests:
            self.queue_load.put(request)

        # Attendre que toutes les images soient chargÃ©es
        self.queue_load.join()

        # Attendre que toutes les Ã©tapes suivantes se terminent
        for queue in [self.queue_resize, self.queue_filter, self.queue_save]:
            queue.join()

    def stop(self):
        """ArrÃªte le pipeline."""
        print("ğŸ›‘ ArrÃªt du pipeline...")

        # Envoyer signal d'arrÃªt Ã  chaque Ã©tape
        for stage in self.stages:
            stage.active = False
            stage.input_queue.put(None)  # Signal d'arrÃªt

        # Attendre la fin des threads
        for thread in self.threads:
            thread.join(timeout=2)

        self.print_stats()

    def print_stats(self):
        """Affiche les statistiques du pipeline."""
        print(f"\nğŸ“Š STATISTIQUES DU PIPELINE")
        for stage in self.stages:
            print(f"{stage.name}: {stage.stats['processed']} traitÃ©s, {stage.stats['errors']} erreurs")

def demo_pipeline():
    """DÃ©monstration du pattern Pipeline."""
    print("ğŸ­ PATTERN PIPELINE")
    print("-" * 20)

    # CrÃ©er des demandes d'images
    image_requests = [
        {'id': f'img_{i}', 'filename': f'photo_{i}.jpg'}
        for i in range(1, 9)
    ]

    pipeline = ImagePipeline()

    try:
        pipeline.start()
        time.sleep(1)  # Laisser le pipeline se stabiliser

        start_time = time.time()
        pipeline.process_images(image_requests)
        total_time = time.time() - start_time

        print(f"\nâ±ï¸ Traitement terminÃ© en {total_time:.2f}s")

    finally:
        pipeline.stop()

demo_pipeline()
```

## Pattern Map-Reduce

Diviser une tÃ¢che en sous-tÃ¢ches, les traiter en parallÃ¨le, puis combiner les rÃ©sultats.

### Map-Reduce pour analyse de texte

```python
import concurrent.futures
import time
import random
from collections import defaultdict, Counter
import re

class MapReduce:
    """ImplÃ©mentation gÃ©nÃ©rique du pattern Map-Reduce."""

    def __init__(self, max_workers=4):
        self.max_workers = max_workers

    def map_phase(self, data_chunks, map_func):
        """Phase Map: traite les chunks en parallÃ¨le."""
        print(f"ğŸ—ºï¸ Phase MAP: {len(data_chunks)} chunks avec {self.max_workers} workers")

        map_results = []

        with concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # Soumettre tous les chunks
            futures = [executor.submit(map_func, chunk) for chunk in data_chunks]

            # RÃ©cupÃ©rer les rÃ©sultats
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                try:
                    result = future.result()
                    map_results.append(result)
                    print(f"âœ… Chunk {i+1} traitÃ©")
                except Exception as e:
                    print(f"âŒ Erreur chunk {i+1}: {e}")

        return map_results

    def reduce_phase(self, map_results, reduce_func):
        """Phase Reduce: combine les rÃ©sultats."""
        print(f"ğŸ”„ Phase REDUCE: Combinaison de {len(map_results)} rÃ©sultats")

        if not map_results:
            return None

        # RÃ©duction sÃ©quentielle (peut Ãªtre parallÃ©lisÃ©e pour de gros volumes)
        result = map_results[0]
        for other_result in map_results[1:]:
            result = reduce_func(result, other_result)

        return result

    def execute(self, data, chunk_size, map_func, reduce_func):
        """ExÃ©cute le processus Map-Reduce complet."""
        print(f"ğŸš€ DÃ‰BUT MAP-REDUCE")
        print(f"DonnÃ©es: {len(data)} Ã©lÃ©ments, chunks de {chunk_size}")

        start_time = time.time()

        # 1. Diviser les donnÃ©es en chunks
        chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]
        print(f"ğŸ“¦ {len(chunks)} chunks crÃ©Ã©s")

        # 2. Phase Map
        map_results = self.map_phase(chunks, map_func)

        # 3. Phase Reduce
        final_result = self.reduce_phase(map_results, reduce_func)

        total_time = time.time() - start_time
        print(f"â±ï¸ Map-Reduce terminÃ© en {total_time:.2f}s")

        return final_result

# Fonctions pour l'analyse de texte
def map_word_count(text_chunk):
    """Fonction Map: compte les mots dans un chunk de texte."""
    word_count = defaultdict(int)

    for line in text_chunk:
        # Nettoyer et diviser en mots
        words = re.findall(r'\b\w+\b', line.lower())
        for word in words:
            word_count[word] += 1

    return dict(word_count)

def reduce_word_count(count1, count2):
    """Fonction Reduce: combine deux dictionnaires de comptage."""
    result = defaultdict(int)

    # Additionner les comptages
    for word, count in count1.items():
        result[word] += count

    for word, count in count2.items():
        result[word] += count

    return dict(result)

def demo_map_reduce():
    """DÃ©monstration du pattern Map-Reduce."""
    print("ğŸŒ PATTERN MAP-REDUCE")
    print("-" * 25)

    # CrÃ©er un corpus de texte d'exemple
    sample_texts = [
        "Python est un langage de programmation puissant et facile Ã  apprendre.",
        "La programmation concurrente amÃ©liore les performances des applications.",
        "Les patterns de concurrence sont des solutions Ã©prouvÃ©es et efficaces.",
        "Python offre plusieurs outils pour la programmation parallÃ¨le et asynchrone.",
        "Map-Reduce est un pattern populaire pour traiter de gros volumes de donnÃ©es.",
        "Les threads et les processus permettent d'exÃ©cuter du code en parallÃ¨le.",
        "La synchronisation est cruciale pour Ã©viter les conditions de course.",
        "Les queues facilitent la communication entre threads et processus.",
    ] * 100  # Multiplier pour avoir plus de donnÃ©es

    # Ajouter du bruit pour rendre plus rÃ©aliste
    for i in range(200):
        words = ["donnÃ©es", "traitement", "performance", "code", "systÃ¨me", "application"]
        sample_texts.append(" ".join(random.choices(words, k=random.randint(3, 8))))

    print(f"ğŸ“– Corpus: {len(sample_texts)} lignes de texte")

    # ExÃ©cuter Map-Reduce
    mr = MapReduce(max_workers=3)
    word_counts = mr.execute(
        data=sample_texts,
        chunk_size=100,
        map_func=map_word_count,
        reduce_func=reduce_word_count
    )

    # Analyser les rÃ©sultats
    if word_counts:
        total_words = sum(word_counts.values())
        unique_words = len(word_counts)

        print(f"\nğŸ“Š RÃ‰SULTATS D'ANALYSE")
        print(f"Mots uniques: {unique_words}")
        print(f"Total des mots: {total_words}")

        # Top 10 des mots les plus frÃ©quents
        top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        print(f"\nTop 10 des mots:")
        for word, count in top_words:
            print(f"  {word}: {count}")

if __name__ == "__main__":
    demo_map_reduce()
```

## Pattern Circuit Breaker

Protection contre les dÃ©faillances en cascade en "coupant le circuit" quand un service est dÃ©faillant.

### Circuit Breaker pour services externes

```python
import threading
import time
import random
from enum import Enum
from datetime import datetime, timedelta

class CircuitState(Enum):
    CLOSED = "closed"      # Normal: requÃªtes passent
    OPEN = "open"          # DÃ©faillant: requÃªtes bloquÃ©es
    HALF_OPEN = "half_open"  # Test: quelques requÃªtes passent

class CircuitBreaker:
    """ImplÃ©mentation du pattern Circuit Breaker."""

    def __init__(self, failure_threshold=5, timeout=60, expected_exception=Exception):
        self.failure_threshold = failure_threshold  # Seuil d'Ã©checs
        self.timeout = timeout  # Temps avant retry (secondes)
        self.expected_exception = expected_exception

        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

        self.lock = threading.Lock()
        self.stats = {
            'total_calls': 0,
            'successful_calls': 0,
            'failed_calls': 0,
            'circuit_open_calls': 0
        }

    def call(self, func, *args, **kwargs):
        """ExÃ©cute une fonction protÃ©gÃ©e par le circuit breaker."""
        with self.lock:
            self.stats['total_calls'] += 1

            # VÃ©rifier l'Ã©tat du circuit
            if self.state == CircuitState.OPEN:
                if self._should_attempt_reset():
                    self.state = CircuitState.HALF_OPEN
                    print(f"ğŸ”„ Circuit HALF-OPEN: Test de rÃ©cupÃ©ration")
                else:
                    self.stats['circuit_open_calls'] += 1
                    print(f"âš¡ Circuit OPEN: Appel bloquÃ©")
                    raise Exception("Circuit breaker OPEN")

            # Tenter l'appel
            try:
                result = func(*args, **kwargs)
                self._on_success()
                return result

            except self.expected_exception as e:
                self._on_failure()
                raise e

    def _should_attempt_reset(self):
        """VÃ©rifie si on peut tenter de rÃ©initialiser le circuit."""
        if self.last_failure_time is None:
            return True

        return datetime.now() - self.last_failure_time > timedelta(seconds=self.timeout)

    def _on_success(self):
        """AppelÃ© lors d'un succÃ¨s."""
        self.stats['successful_calls'] += 1

        if self.state == CircuitState.HALF_OPEN:
            self.state = CircuitState.CLOSED
            self.failure_count = 0
            print(f"âœ… Circuit CLOSED: Service rÃ©cupÃ©rÃ©")

    def _on_failure(self):
        """AppelÃ© lors d'un Ã©chec."""
        self.stats['failed_calls'] += 1
        self.failure_count += 1
        self.last_failure_time = datetime.now()

        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN
            print(f"âš¡ Circuit OPEN: Trop d'Ã©checs ({self.failure_count})")

    def get_stats(self):
        """Retourne les statistiques."""
        with self.lock:
            stats = self.stats.copy()
            stats['state'] = self.state.value
            stats['failure_count'] = self.failure_count
            return stats

class ServiceExterne:
    """Simule un service externe instable."""

    def __init__(self, nom, taux_echec=0.3):
        self.nom = nom
        self.taux_echec = taux_echec
        self.appels = 0

    def appeler_api(self, donnees):
        """Simule un appel API qui peut Ã©chouer."""
        self.appels += 1

        # Simuler latence rÃ©seau
        time.sleep(random.uniform(0.1, 0.3))

        # Simuler des Ã©checs
        if random.random() < self.taux_echec:
            raise Exception(f"{self.nom}: Service temporairement indisponible")

        return f"{self.nom}: DonnÃ©es traitÃ©es - {donnees}"

def demo_circuit_breaker():
    """DÃ©monstration du pattern Circuit Breaker."""
    print("âš¡ PATTERN CIRCUIT BREAKER")
    print("-" * 30)

    # CrÃ©er un service instable
    service = ServiceExterne("API-Externe", taux_echec=0.4)

    # CrÃ©er un circuit breaker
    circuit = CircuitBreaker(
        failure_threshold=3,
        timeout=5,
        expected_exception=Exception
    )

    # Simuler des appels clients
    def client_worker(nom_client, nb_appels):
        """Simule un client qui fait des appels."""
        print(f"ğŸ‘¤ {nom_client} dÃ©marrÃ©")

        for i in range(nb_appels):
            try:
                donnees = f"requete_{nom_client}_{i+1}"
                result = circuit.call(service.appeler_api, donnees)
                print(f"âœ… {nom_client}: {result}")

            except Exception as e:
                print(f"âŒ {nom_client}: {e}")

            time.sleep(random.uniform(0.5, 1.5))

        print(f"ğŸ {nom_client} terminÃ©")

   # Lancer plusieurs clients
    clients = ["Client-A", "Client-B", "Client-C"]
    threads = []

    for client in clients:
        t = threading.Thread(target=client_worker, args=(client, 8))
        t.start()
        threads.append(t)

    # Attendre la fin
    for t in threads:
        t.join()

    # Afficher les statistiques finales
    stats = circuit.get_stats()
    print(f"\nğŸ“Š STATISTIQUES CIRCUIT BREAKER")
    print(f"Ã‰tat final: {stats['state'].upper()}")
    print(f"Appels totaux: {stats['total_calls']}")
    print(f"SuccÃ¨s: {stats['successful_calls']}")
    print(f"Ã‰checs: {stats['failed_calls']}")
    print(f"BloquÃ©s (circuit ouvert): {stats['circuit_open_calls']}")
    print(f"Taux de succÃ¨s: {stats['successful_calls']/stats['total_calls']*100:.1f}%")

demo_circuit_breaker()
```

## Pattern Scatter-Gather

Diffuser une requÃªte vers plusieurs services, puis rassembler les rÃ©ponses.

### Scatter-Gather pour recherche distribuÃ©e

```python
import concurrent.futures
import time
import random
import threading

class SearchService:
    """Service de recherche simulÃ©."""

    def __init__(self, name, latency_range=(0.1, 1.0), error_rate=0.1):
        self.name = name
        self.latency_range = latency_range
        self.error_rate = error_rate

    def search(self, query):
        """Effectue une recherche."""
        # Simuler latence variable
        time.sleep(random.uniform(*self.latency_range))

        # Simuler des erreurs occasionnelles
        if random.random() < self.error_rate:
            raise Exception(f"{self.name}: Service temporairement indisponible")

        # GÃ©nÃ©rer des rÃ©sultats simulÃ©s
        results = []
        num_results = random.randint(0, 5)

        for i in range(num_results):
            results.append({
                'title': f"RÃ©sultat {i+1} pour '{query}'",
                'url': f"https://{self.name.lower()}.com/result_{i+1}",
                'score': random.uniform(0.1, 1.0),
                'source': self.name
            })

        return {
            'service': self.name,
            'query': query,
            'results': results,
            'count': len(results)
        }

class ScatterGather:
    """ImplÃ©mentation du pattern Scatter-Gather."""

    def __init__(self, services, timeout=3.0):
        self.services = services
        self.timeout = timeout

    def search_all(self, query, min_responses=1):
        """Lance une recherche sur tous les services."""
        print(f"ğŸ” Recherche '{query}' sur {len(self.services)} services")

        start_time = time.time()
        results = []
        errors = []

        # Phase Scatter: lancer toutes les requÃªtes en parallÃ¨le
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.services)) as executor:
            # Soumettre toutes les recherches
            future_to_service = {
                executor.submit(service.search, query): service
                for service in self.services
            }

            # Phase Gather: collecter les rÃ©ponses
            completed = 0
            for future in concurrent.futures.as_completed(future_to_service, timeout=self.timeout):
                service = future_to_service[future]

                try:
                    result = future.result()
                    results.append(result)
                    completed += 1

                    print(f"âœ… {service.name}: {result['count']} rÃ©sultats "
                          f"({time.time() - start_time:.2f}s)")

                    # ArrÃªter dÃ¨s qu'on a assez de rÃ©ponses (optionnel)
                    if completed >= min_responses and len(results) >= len(self.services) // 2:
                        break

                except Exception as e:
                    errors.append({
                        'service': service.name,
                        'error': str(e)
                    })
                    print(f"âŒ {service.name}: {e}")

        total_time = time.time() - start_time

        return {
            'query': query,
            'results': results,
            'errors': errors,
            'total_time': total_time,
            'services_responded': len(results),
            'services_failed': len(errors)
        }

    def aggregate_results(self, search_response):
        """AgrÃ¨ge et trie les rÃ©sultats de tous les services."""
        all_results = []

        # Collecter tous les rÃ©sultats
        for service_response in search_response['results']:
            for result in service_response['results']:
                all_results.append(result)

        # Trier par score dÃ©croissant
        all_results.sort(key=lambda x: x['score'], reverse=True)

        return {
            'query': search_response['query'],
            'total_results': len(all_results),
            'results': all_results[:10],  # Top 10
            'services_used': len(search_response['results']),
            'response_time': search_response['total_time']
        }

def demo_scatter_gather():
    """DÃ©monstration du pattern Scatter-Gather."""
    print("ğŸ“¡ PATTERN SCATTER-GATHER")
    print("-" * 30)

    # CrÃ©er plusieurs services de recherche avec caractÃ©ristiques diffÃ©rentes
    services = [
        SearchService("GoogleSearch", latency_range=(0.1, 0.3), error_rate=0.05),
        SearchService("BingSearch", latency_range=(0.2, 0.5), error_rate=0.1),
        SearchService("DuckDuckGo", latency_range=(0.3, 0.8), error_rate=0.15),
        SearchService("YahooSearch", latency_range=(0.4, 1.2), error_rate=0.2),
        SearchService("LocalSearch", latency_range=(0.1, 0.2), error_rate=0.3)
    ]

    scatter_gather = ScatterGather(services, timeout=2.0)

    # Effectuer plusieurs recherches
    queries = ["python concurrence", "machine learning", "web development"]

    for query in queries:
        print(f"\n{'='*50}")

        # Recherche distribuÃ©e
        search_response = scatter_gather.search_all(query, min_responses=2)

        # AgrÃ©gation des rÃ©sultats
        aggregated = scatter_gather.aggregate_results(search_response)

        print(f"\nğŸ“Š RÃ‰SULTATS POUR '{query}'")
        print(f"Services rÃ©pondus: {search_response['services_responded']}/{len(services)}")
        print(f"Temps de rÃ©ponse: {search_response['total_time']:.2f}s")
        print(f"Total rÃ©sultats: {aggregated['total_results']}")

        if aggregated['results']:
            print(f"\nTop 3 rÃ©sultats:")
            for i, result in enumerate(aggregated['results'][:3], 1):
                print(f"  {i}. {result['title']} (score: {result['score']:.2f}) - {result['source']}")

        if search_response['errors']:
            print(f"\nErreurs:")
            for error in search_response['errors']:
                print(f"  âŒ {error['service']}: {error['error']}")

        time.sleep(1)  # Pause entre les recherches

demo_scatter_gather()
```

## Exercices pratiques intÃ©grÃ©s

### Exercice 1 : SystÃ¨me de cache distribuÃ©

```python
import threading
import time
import random
import hashlib
from collections import defaultdict

class DistributedCache:
    """Cache distribuÃ© utilisant plusieurs patterns."""

    def __init__(self, num_shards=3, max_size_per_shard=100):
        self.num_shards = num_shards
        self.max_size_per_shard = max_size_per_shard

        # Shards du cache (pattern partitioning)
        self.shards = [
            {'data': {}, 'lock': threading.RLock(), 'access_count': 0}
            for _ in range(num_shards)
        ]

        # Pool de workers pour les opÃ©rations asynchrones
        self.task_queue = queue.Queue()
        self.workers = []
        self.active = True

        # Statistiques globales
        self.global_stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'evictions': 0
        }
        self.stats_lock = threading.Lock()

    def _hash_key(self, key):
        """Hash une clÃ© pour dÃ©terminer le shard."""
        return int(hashlib.md5(key.encode()).hexdigest(), 16) % self.num_shards

    def _get_shard(self, key):
        """Retourne le shard appropriÃ© pour une clÃ©."""
        shard_id = self._hash_key(key)
        return self.shards[shard_id], shard_id

    def get(self, key):
        """RÃ©cupÃ¨re une valeur du cache."""
        shard, shard_id = self._get_shard(key)

        with shard['lock']:
            shard['access_count'] += 1

            if key in shard['data']:
                entry = shard['data'][key]

                # VÃ©rifier l'expiration
                if entry['expires_at'] > time.time():
                    with self.stats_lock:
                        self.global_stats['hits'] += 1

                    print(f"ğŸ’¾ Cache HIT: {key} (shard {shard_id})")
                    return entry['value']
                else:
                    # ExpirÃ©, supprimer
                    del shard['data'][key]

            with self.stats_lock:
                self.global_stats['misses'] += 1

            print(f"âŒ Cache MISS: {key} (shard {shard_id})")
            return None

    def set(self, key, value, ttl=300):
        """Stocke une valeur dans le cache."""
        shard, shard_id = self._get_shard(key)

        with shard['lock']:
            # Ã‰viction si nÃ©cessaire
            if len(shard['data']) >= self.max_size_per_shard and key not in shard['data']:
                self._evict_lru(shard)

            # Stocker la valeur
            shard['data'][key] = {
                'value': value,
                'expires_at': time.time() + ttl,
                'created_at': time.time()
            }

            with self.stats_lock:
                self.global_stats['sets'] += 1

            print(f"ğŸ’¾ Cache SET: {key} (shard {shard_id}, TTL: {ttl}s)")

    def _evict_lru(self, shard):
        """Ã‰victe l'Ã©lÃ©ment le moins rÃ©cemment utilisÃ©."""
        if not shard['data']:
            return

        # Trouver l'entrÃ©e la plus ancienne
        oldest_key = min(shard['data'].keys(),
                        key=lambda k: shard['data'][k]['created_at'])

        del shard['data'][oldest_key]

        with self.stats_lock:
            self.global_stats['evictions'] += 1

        print(f"ğŸ—‘ï¸ Ã‰viction LRU: {oldest_key}")

    def start_background_workers(self, num_workers=2):
        """DÃ©marre les workers pour les tÃ¢ches de maintenance."""
        for i in range(num_workers):
            worker = threading.Thread(
                target=self._background_worker,
                args=(f"Worker-{i+1}",)
            )
            worker.start()
            self.workers.append(worker)

        print(f"ğŸ”§ {num_workers} workers de maintenance dÃ©marrÃ©s")

    def _background_worker(self, name):
        """Worker qui nettoie les entrÃ©es expirÃ©es."""
        while self.active:
            try:
                # Nettoyer pÃ©riodiquement
                time.sleep(5)
                self._cleanup_expired()

            except Exception as e:
                print(f"âŒ {name}: Erreur - {e}")

    def _cleanup_expired(self):
        """Nettoie les entrÃ©es expirÃ©es."""
        now = time.time()
        total_cleaned = 0

        for shard_id, shard in enumerate(self.shards):
            with shard['lock']:
                expired_keys = [
                    key for key, entry in shard['data'].items()
                    if entry['expires_at'] <= now
                ]

                for key in expired_keys:
                    del shard['data'][key]
                    total_cleaned += 1

        if total_cleaned > 0:
            print(f"ğŸ§¹ Nettoyage: {total_cleaned} entrÃ©es expirÃ©es supprimÃ©es")

    def get_stats(self):
        """Retourne les statistiques du cache."""
        with self.stats_lock:
            stats = self.global_stats.copy()

        # Ajouter les stats par shard
        shard_stats = []
        for i, shard in enumerate(self.shards):
            with shard['lock']:
                shard_stats.append({
                    'shard_id': i,
                    'size': len(shard['data']),
                    'access_count': shard['access_count']
                })

        stats['shards'] = shard_stats
        stats['total_size'] = sum(s['size'] for s in shard_stats)

        return stats

    def stop(self):
        """ArrÃªte le cache et ses workers."""
        self.active = False

        for worker in self.workers:
            worker.join(timeout=1)

        print("ğŸ›‘ Cache arrÃªtÃ©")

def demo_distributed_cache():
    """Test du cache distribuÃ©."""
    print("ğŸ—„ï¸ CACHE DISTRIBUÃ‰")
    print("-" * 20)

    cache = DistributedCache(num_shards=3, max_size_per_shard=5)
    cache.start_background_workers(2)

    def cache_user(name, operations):
        """Simule un utilisateur du cache."""
        for i in range(operations):
            # 70% lecture, 30% Ã©criture
            if random.random() < 0.7:
                # Lecture
                key = f"key_{random.randint(1, 10)}"
                value = cache.get(key)
            else:
                # Ã‰criture
                key = f"key_{random.randint(1, 15)}"
                value = f"value_{name}_{i}"
                ttl = random.randint(5, 20)
                cache.set(key, value, ttl)

            time.sleep(random.uniform(0.1, 0.3))

    # Lancer plusieurs utilisateurs
    users = ["User-A", "User-B", "User-C"]
    threads = []

    for user in users:
        t = threading.Thread(target=cache_user, args=(user, 12))
        t.start()
        threads.append(t)

    # Attendre et afficher les stats pÃ©riodiquement
    for i in range(3):
        time.sleep(4)
        stats = cache.get_stats()

        print(f"\nğŸ“Š STATS aprÃ¨s {(i+1)*4}s:")
        print(f"Hits/Misses: {stats['hits']}/{stats['misses']}")
        print(f"Sets/Evictions: {stats['sets']}/{stats['evictions']}")
        print(f"Taille totale: {stats['total_size']}")

        for shard in stats['shards']:
            print(f"  Shard {shard['shard_id']}: {shard['size']} entrÃ©es, {shard['access_count']} accÃ¨s")

    # Attendre la fin
    for t in threads:
        t.join()

    cache.stop()

demo_distributed_cache()
```

### Exercice 2 : Orchestrateur de tÃ¢ches

```python
import threading
import queue
import time
import random
from enum import Enum
from dataclasses import dataclass
from typing import List, Callable, Any

class TaskStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class Task:
    """ReprÃ©sente une tÃ¢che dans l'orchestrateur."""
    id: str
    func: Callable
    args: tuple = ()
    kwargs: dict = None
    dependencies: List[str] = None
    priority: int = 0
    timeout: float = 30.0
    retry_count: int = 0
    max_retries: int = 2

    def __post_init__(self):
        if self.kwargs is None:
            self.kwargs = {}
        if self.dependencies is None:
            self.dependencies = []

class TaskOrchestrator:
    """Orchestrateur de tÃ¢ches avec gestion des dÃ©pendances."""

    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.tasks = {}  # task_id -> Task
        self.task_status = {}  # task_id -> TaskStatus
        self.task_results = {}  # task_id -> result
        self.ready_queue = queue.PriorityQueue()

        self.workers = []
        self.active = False
        self.lock = threading.RLock()

        # Ã‰vÃ©nements pour notification
        self.task_completed_event = threading.Event()

    def add_task(self, task: Task):
        """Ajoute une tÃ¢che Ã  l'orchestrateur."""
        with self.lock:
            self.tasks[task.id] = task
            self.task_status[task.id] = TaskStatus.PENDING

            print(f"ğŸ“‹ TÃ¢che ajoutÃ©e: {task.id} (prioritÃ©: {task.priority})")

            # VÃ©rifier si la tÃ¢che peut Ãªtre exÃ©cutÃ©e immÃ©diatement
            self._check_ready_tasks()

    def _check_ready_tasks(self):
        """VÃ©rifie quelles tÃ¢ches peuvent Ãªtre exÃ©cutÃ©es."""
        for task_id, task in self.tasks.items():
            if self.task_status[task_id] == TaskStatus.PENDING:
                if self._dependencies_satisfied(task):
                    # Ajouter Ã  la queue de prioritÃ© (nÃ©gatif pour ordre dÃ©croissant)
                    self.ready_queue.put((-task.priority, task_id))
                    self.task_status[task_id] = TaskStatus.PENDING
                    print(f"âœ… TÃ¢che prÃªte: {task_id}")

    def _dependencies_satisfied(self, task: Task) -> bool:
        """VÃ©rifie si toutes les dÃ©pendances sont satisfaites."""
        for dep_id in task.dependencies:
            if dep_id not in self.task_status:
                return False
            if self.task_status[dep_id] != TaskStatus.COMPLETED:
                return False
        return True

    def start(self):
        """DÃ©marre l'orchestrateur."""
        if self.active:
            return

        print(f"ğŸš€ DÃ©marrage orchestrateur ({self.max_workers} workers)")
        self.active = True

        # DÃ©marrer les workers
        for i in range(self.max_workers):
            worker = threading.Thread(target=self._worker, args=(f"Worker-{i+1}",))
            worker.start()
            self.workers.append(worker)

        # Thread de monitoring des dÃ©pendances
        monitor = threading.Thread(target=self._dependency_monitor)
        monitor.start()
        self.workers.append(monitor)

    def _worker(self, name):
        """Worker qui exÃ©cute les tÃ¢ches."""
        print(f"ğŸ‘· {name} dÃ©marrÃ©")

        while self.active:
            try:
                # Prendre une tÃ¢che (timeout pour vÃ©rifier l'arrÃªt)
                try:
                    priority, task_id = self.ready_queue.get(timeout=1)
                except queue.Empty:
                    continue

                # ExÃ©cuter la tÃ¢che
                self._execute_task(task_id, name)
                self.ready_queue.task_done()

            except Exception as e:
                print(f"âŒ {name}: Erreur - {e}")

        print(f"ğŸ›‘ {name} arrÃªtÃ©")

    def _execute_task(self, task_id, worker_name):
        """ExÃ©cute une tÃ¢che spÃ©cifique."""
        with self.lock:
            if self.task_status[task_id] != TaskStatus.PENDING:
                return  # TÃ¢che dÃ©jÃ  prise par un autre worker

            self.task_status[task_id] = TaskStatus.RUNNING
            task = self.tasks[task_id]

        print(f"ğŸ”„ {worker_name}: ExÃ©cute {task_id}")

        try:
            # PrÃ©parer les arguments avec les rÃ©sultats des dÃ©pendances
            kwargs = task.kwargs.copy()

            # Ajouter les rÃ©sultats des dÃ©pendances
            for dep_id in task.dependencies:
                if dep_id in self.task_results:
                    kwargs[f"dep_{dep_id}"] = self.task_results[dep_id]

            # ExÃ©cuter avec timeout
            start_time = time.time()
            result = task.func(*task.args, **kwargs)
            execution_time = time.time() - start_time

            # Marquer comme terminÃ©e
            with self.lock:
                self.task_status[task_id] = TaskStatus.COMPLETED
                self.task_results[task_id] = result

            print(f"âœ… {worker_name}: {task_id} terminÃ©e ({execution_time:.2f}s)")

            # Notifier que des tÃ¢ches peuvent Ãªtre dÃ©bloquÃ©es
            self.task_completed_event.set()

        except Exception as e:
            print(f"âŒ {worker_name}: {task_id} Ã©chouÃ©e - {e}")

            # Gestion des retry
            task.retry_count += 1
            if task.retry_count <= task.max_retries:
                print(f"ğŸ”„ {task_id}: Retry {task.retry_count}/{task.max_retries}")
                with self.lock:
                    self.task_status[task_id] = TaskStatus.PENDING
                    self.ready_queue.put((-task.priority, task_id))
            else:
                with self.lock:
                    self.task_status[task_id] = TaskStatus.FAILED
                print(f"ğŸ’¥ {task_id}: Ã‰chec dÃ©finitif aprÃ¨s {task.max_retries} tentatives")

    def _dependency_monitor(self):
        """Monitore les changements pour dÃ©bloquer de nouvelles tÃ¢ches."""
        while self.active:
            self.task_completed_event.wait(timeout=1)
            self.task_completed_event.clear()

            # VÃ©rifier s'il y a de nouvelles tÃ¢ches Ã  dÃ©bloquer
            with self.lock:
                self._check_ready_tasks()

    def wait_all(self, timeout=None):
        """Attend que toutes les tÃ¢ches se terminent."""
        start_time = time.time()

        while self.active:
            with self.lock:
                pending = sum(1 for status in self.task_status.values()
                            if status in [TaskStatus.PENDING, TaskStatus.RUNNING])

                if pending == 0:
                    break

            if timeout and (time.time() - start_time) > timeout:
                print("â° Timeout atteint")
                break

            time.sleep(0.5)

        self.stop()

    def stop(self):
        """ArrÃªte l'orchestrateur."""
        print("ğŸ›‘ ArrÃªt de l'orchestrateur...")
        self.active = False

        for worker in self.workers:
            worker.join(timeout=2)

        self.print_summary()

    def print_summary(self):
        """Affiche un rÃ©sumÃ© de l'exÃ©cution."""
        with self.lock:
            status_counts = {}
            for status in self.task_status.values():
                status_counts[status] = status_counts.get(status, 0) + 1

        print(f"\nğŸ“Š RÃ‰SUMÃ‰ DE L'ORCHESTRATION")
        print(f"Total tÃ¢ches: {len(self.tasks)}")
        for status, count in status_counts.items():
            print(f"{status.value}: {count}")

# Fonctions de test pour l'orchestrateur
def task_a():
    """TÃ¢che A - Pas de dÃ©pendances."""
    time.sleep(random.uniform(1, 2))
    return "RÃ©sultat de A"

def task_b():
    """TÃ¢che B - Pas de dÃ©pendances."""
    time.sleep(random.uniform(0.5, 1.5))
    return "RÃ©sultat de B"

def task_c(dep_task_a, dep_task_b):
    """TÃ¢che C - DÃ©pend de A et B."""
    time.sleep(random.uniform(0.8, 1.2))
    return f"RÃ©sultat de C (utilise {dep_task_a} et {dep_task_b})"

def task_d(dep_task_c):
    """TÃ¢che D - DÃ©pend de C."""
    time.sleep(random.uniform(0.3, 0.8))

    # Simuler un Ã©chec occasionnel
    if random.random() < 0.3:
        raise Exception("Ã‰chec simulÃ© de la tÃ¢che D")

    return f"RÃ©sultat de D (utilise {dep_task_c})"

def demo_task_orchestrator():
    """DÃ©monstration de l'orchestrateur de tÃ¢ches."""
    print("ğŸ­ ORCHESTRATEUR DE TÃ‚CHES")
    print("-" * 30)

    orchestrator = TaskOrchestrator(max_workers=2)

    # DÃ©finir les tÃ¢ches avec dÃ©pendances
    tasks = [
        Task("task_a", task_a, priority=1),
        Task("task_b", task_b, priority=1),
        Task("task_c", task_c, dependencies=["task_a", "task_b"], priority=2),
        Task("task_d", task_d, dependencies=["task_c"], priority=3, max_retries=3),
        Task("task_e", lambda: "RÃ©sultat E", priority=1),  # TÃ¢che indÃ©pendante
    ]

    # DÃ©marrer l'orchestrateur
    orchestrator.start()

    # Ajouter les tÃ¢ches
    for task in tasks:
        orchestrator.add_task(task)
        time.sleep(0.2)  # Petit dÃ©lai pour voir l'ordre

    # Attendre la fin
    orchestrator.wait_all(timeout=30)

demo_task_orchestrator()
```

## RÃ©capitulatif des patterns

### Tableau de comparaison

| Pattern | Usage principal | Avantages | InconvÃ©nients |
|---------|-----------------|-----------|---------------|
| **Producer-Consumer** | Communication async | DÃ©couplage, gestion de charge | ComplexitÃ© de synchronisation |
| **Worker Pool** | Traitement parallÃ¨le | Utilisation optimale des ressources | Pas de dÃ©pendances entre tÃ¢ches |
| **Pipeline** | Traitement sÃ©quentiel | DÃ©bit Ã©levÃ©, spÃ©cialisation | SÃ©quentiel, point de bottleneck |
| **Map-Reduce** | Gros volumes de donnÃ©es | ScalabilitÃ© horizontale | Overhead de coordination |
| **Circuit Breaker** | RÃ©silience des services | Protection contre les cascades | Faux positifs possibles |
| **Scatter-Gather** | Recherche distribuÃ©e | Latence rÃ©duite, redondance | ComplexitÃ© de gestion |

### Conseils de choix

```python
# âœ… Producer-Consumer : Communication asynchrone
if "communication" in requirements and "dÃ©couplage" in requirements:
    use_producer_consumer()

# âœ… Worker Pool : TÃ¢ches indÃ©pendantes parallÃ¨les
if "parallÃ©lisme" in requirements and "tÃ¢ches_indÃ©pendantes" in requirements:
    use_worker_pool()

# âœ… Pipeline : Traitement en Ã©tapes
if "Ã©tapes_sÃ©quentielles" in requirements and "dÃ©bit" in requirements:
    use_pipeline()

# âœ… Map-Reduce : Gros volumes de donnÃ©es
if "big_data" in requirements and "calcul_distribuÃ©" in requirements:
    use_map_reduce()
```

## Bonnes pratiques gÃ©nÃ©rales

### **1. Choisir le bon pattern**
```python
# Analyser le problÃ¨me avant de choisir
- Type de donnÃ©es (flux vs batch)
- DÃ©pendances entre tÃ¢ches
- Contraintes de performance
- Niveau de parallÃ©lisme souhaitÃ©
```

### **2. Monitoring et observabilitÃ©**
```python
# Toujours inclure des mÃ©triques
class PatternWithMetrics:
    def __init__(self):
        self.metrics = {
            'processed': 0,
            'errors': 0,
            'avg_time': 0
        }

    def track_operation(self, duration, success):
        self.metrics['processed'] += 1
        if not success:
            self.metrics['errors'] += 1
```

### **3. Gestion d'erreurs robuste**
```python
# GÃ©rer les pannes gracieusement
try:
    result = process_task(task)
except RecoverableError:
    retry_task(task)
except FatalError:
    mark_failed(task)
    notify_admin()
```

### **4. Timeouts et annulation**
```python
# Toujours inclure des timeouts
import signal
from contextlib import contextmanager

@contextmanager
def timeout(seconds):
    def timeout_handler(signum, frame):
        raise TimeoutError(f"OpÃ©ration dÃ©passÃ©e ({seconds}s)")

    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(seconds)

    try:
        yield
    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)

# Usage
try:
    with timeout(30):
        result = long_running_operation()
except TimeoutError:
    print("OpÃ©ration annulÃ©e (timeout)")
```

### **5. Backpressure et contrÃ´le de flux**
```python
# Ã‰viter la surcharge avec backpressure
class BackpressureQueue:
    def __init__(self, maxsize, backpressure_threshold=0.8):
        self.queue = queue.Queue(maxsize)
        self.threshold = int(maxsize * backpressure_threshold)

    def put(self, item, timeout=None):
        if self.queue.qsize() > self.threshold:
            print("âš ï¸ Backpressure activÃ©e - ralentissement")
            time.sleep(0.1)  # Ralentir le producteur

        return self.queue.put(item, timeout=timeout)
```

## Projet final : SystÃ¨me de traitement de donnÃ©es distribuÃ©

CrÃ©ons un systÃ¨me complet qui combine tous les patterns appris.

### Architecture du systÃ¨me

```python
import threading
import queue
import time
import random
import json
import logging
from datetime import datetime
from enum import Enum
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Callable
import concurrent.futures

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class JobType(Enum):
    DATA_INGESTION = "data_ingestion"
    DATA_PROCESSING = "data_processing"
    DATA_ANALYSIS = "data_analysis"
    REPORT_GENERATION = "report_generation"

class JobStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class Job:
    """ReprÃ©sente un job de traitement."""
    id: str
    type: JobType
    data: Dict[str, Any]
    priority: int = 0
    dependencies: List[str] = None
    timeout: float = 60.0
    max_retries: int = 2
    created_at: float = None

    def __post_init__(self):
        if self.dependencies is None:
            self.dependencies = []
        if self.created_at is None:
            self.created_at = time.time()

class DistributedDataProcessor:
    """SystÃ¨me de traitement de donnÃ©es distribuÃ©."""

    def __init__(self,
                 ingestion_workers=2,
                 processing_workers=3,
                 analysis_workers=2,
                 report_workers=1):

        self.logger = logging.getLogger(self.__class__.__name__)

        # Configuration des workers par type
        self.worker_config = {
            JobType.DATA_INGESTION: ingestion_workers,
            JobType.DATA_PROCESSING: processing_workers,
            JobType.DATA_ANALYSIS: analysis_workers,
            JobType.REPORT_GENERATION: report_workers
        }

        # Queues spÃ©cialisÃ©es pour chaque type de job
        self.job_queues = {
            job_type: queue.PriorityQueue()
            for job_type in JobType
        }

        # Ã‰tat global du systÃ¨me
        self.jobs = {}  # job_id -> Job
        self.job_status = {}  # job_id -> JobStatus
        self.job_results = {}  # job_id -> result
        self.job_errors = {}  # job_id -> error_info

        # Workers par type
        self.workers = {job_type: [] for job_type in JobType}
        self.active = False

        # Synchronisation
        self.lock = threading.RLock()
        self.job_completed_event = threading.Event()

        # Statistiques
        self.stats = {
            'jobs_submitted': 0,
            'jobs_completed': 0,
            'jobs_failed': 0,
            'total_processing_time': 0
        }

        # Circuit breakers pour chaque type de job
        self.circuit_breakers = {
            job_type: CircuitBreaker(failure_threshold=3, timeout=30)
            for job_type in JobType
        }

    def submit_job(self, job: Job) -> bool:
        """Soumet un job pour traitement."""
        with self.lock:
            # VÃ©rifier les dÃ©pendances
            for dep_id in job.dependencies:
                if dep_id not in self.jobs:
                    self.logger.error(f"DÃ©pendance manquante pour {job.id}: {dep_id}")
                    return False

            # Enregistrer le job
            self.jobs[job.id] = job
            self.job_status[job.id] = JobStatus.PENDING
            self.stats['jobs_submitted'] += 1

            self.logger.info(f"Job soumis: {job.id} ({job.type.value})")

            # VÃ©rifier si le job peut Ãªtre exÃ©cutÃ© immÃ©diatement
            self._check_ready_jobs()

            return True

    def _check_ready_jobs(self):
        """VÃ©rifie quels jobs peuvent Ãªtre exÃ©cutÃ©s."""
        for job_id, job in self.jobs.items():
            if self.job_status[job_id] == JobStatus.PENDING:
                if self._dependencies_satisfied(job):
                    # Ajouter Ã  la queue appropriÃ©e
                    priority_item = (-job.priority, time.time(), job_id)
                    self.job_queues[job.type].put(priority_item)
                    self.logger.info(f"Job prÃªt: {job_id}")

    def _dependencies_satisfied(self, job: Job) -> bool:
        """VÃ©rifie si toutes les dÃ©pendances sont satisfaites."""
        for dep_id in job.dependencies:
            if (dep_id not in self.job_status or
                self.job_status[dep_id] != JobStatus.COMPLETED):
                return False
        return True

    def start(self):
        """DÃ©marre le systÃ¨me de traitement."""
        if self.active:
            self.logger.warning("SystÃ¨me dÃ©jÃ  dÃ©marrÃ©")
            return

        self.logger.info("ğŸš€ DÃ©marrage du systÃ¨me de traitement distribuÃ©")
        self.active = True

        # DÃ©marrer les workers pour chaque type de job
        for job_type, worker_count in self.worker_config.items():
            for i in range(worker_count):
                worker_name = f"{job_type.value}-worker-{i+1}"
                worker = threading.Thread(
                    target=self._worker,
                    args=(worker_name, job_type)
                )
                worker.start()
                self.workers[job_type].append(worker)

        # DÃ©marrer le moniteur de dÃ©pendances
        monitor = threading.Thread(target=self._dependency_monitor)
        monitor.start()

        # DÃ©marrer le collecteur de mÃ©triques
        metrics_collector = threading.Thread(target=self._metrics_collector)
        metrics_collector.start()

        self.logger.info("âœ… SystÃ¨me dÃ©marrÃ©")

    def _worker(self, worker_name: str, job_type: JobType):
        """Worker spÃ©cialisÃ© pour un type de job."""
        logger = logging.getLogger(f"Worker.{worker_name}")
        logger.info(f"Worker {worker_name} dÃ©marrÃ©")

        job_queue = self.job_queues[job_type]

        while self.active:
            try:
                # Prendre un job avec timeout
                try:
                    priority, submit_time, job_id = job_queue.get(timeout=1)
                except queue.Empty:
                    continue

                # VÃ©rifier que le job est toujours valide
                with self.lock:
                    if (job_id not in self.job_status or
                        self.job_status[job_id] != JobStatus.PENDING):
                        job_queue.task_done()
                        continue

                    self.job_status[job_id] = JobStatus.RUNNING
                    job = self.jobs[job_id]

                # ExÃ©cuter le job avec circuit breaker
                try:
                    circuit_breaker = self.circuit_breakers[job_type]
                    result = circuit_breaker.call(
                        self._execute_job_by_type,
                        job, worker_name
                    )

                    # Job rÃ©ussi
                    with self.lock:
                        self.job_status[job_id] = JobStatus.COMPLETED
                        self.job_results[job_id] = result
                        self.stats['jobs_completed'] += 1

                    logger.info(f"âœ… Job {job_id} terminÃ© avec succÃ¨s")
                    self.job_completed_event.set()

                except Exception as e:
                    # Job Ã©chouÃ©
                    logger.error(f"âŒ Job {job_id} Ã©chouÃ©: {e}")

                    with self.lock:
                        if job_id not in self.job_errors:
                            self.job_errors[job_id] = []

                        self.job_errors[job_id].append({
                            'error': str(e),
                            'timestamp': time.time(),
                            'worker': worker_name
                        })

                        # Gestion des retry
                        retry_count = len(self.job_errors[job_id])
                        if retry_count <= job.max_retries:
                            logger.info(f"ğŸ”„ Retry {retry_count}/{job.max_retries} pour {job_id}")
                            self.job_status[job_id] = JobStatus.PENDING
                            # Remettre en queue avec prioritÃ© rÃ©duite
                            retry_priority = (-max(job.priority - retry_count, 0), time.time(), job_id)
                            job_queue.put(retry_priority)
                        else:
                            self.job_status[job_id] = JobStatus.FAILED
                            self.stats['jobs_failed'] += 1
                            logger.error(f"ğŸ’¥ Job {job_id} Ã©chec dÃ©finitif")

                job_queue.task_done()

            except Exception as e:
                logger.error(f"Erreur worker {worker_name}: {e}")

        logger.info(f"Worker {worker_name} arrÃªtÃ©")

    def _execute_job_by_type(self, job: Job, worker_name: str) -> Any:
        """ExÃ©cute un job selon son type."""
        start_time = time.time()

        # PrÃ©parer les donnÃ©es avec les rÃ©sultats des dÃ©pendances
        job_data = job.data.copy()
        for dep_id in job.dependencies:
            if dep_id in self.job_results:
                job_data[f'dep_{dep_id}'] = self.job_results[dep_id]

        # ExÃ©cuter selon le type
        if job.type == JobType.DATA_INGESTION:
            result = self._process_ingestion(job_data)
        elif job.type == JobType.DATA_PROCESSING:
            result = self._process_data_processing(job_data)
        elif job.type == JobType.DATA_ANALYSIS:
            result = self._process_analysis(job_data)
        elif job.type == JobType.REPORT_GENERATION:
            result = self._process_report_generation(job_data)
        else:
            raise ValueError(f"Type de job non supportÃ©: {job.type}")

        # Enregistrer le temps de traitement
        processing_time = time.time() - start_time
        with self.lock:
            self.stats['total_processing_time'] += processing_time

        return result

    def _process_ingestion(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Simule l'ingestion de donnÃ©es."""
        time.sleep(random.uniform(0.5, 2.0))  # Simuler I/O

        # Simuler une erreur occasionnelle
        if random.random() < 0.1:
            raise Exception("Erreur d'ingestion de donnÃ©es")

        return {
            'type': 'ingestion_result',
            'records_ingested': random.randint(100, 1000),
            'source': data.get('source', 'unknown'),
            'timestamp': time.time()
        }

    def _process_data_processing(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Simule le traitement de donnÃ©es."""
        time.sleep(random.uniform(1.0, 3.0))  # Simuler calculs

        # Simuler une erreur occasionnelle
        if random.random() < 0.15:
            raise Exception("Erreur de traitement")

        # Utiliser les donnÃ©es d'ingestion si disponibles
        base_records = 500
        for key, value in data.items():
            if key.startswith('dep_') and isinstance(value, dict):
                base_records = value.get('records_ingested', base_records)
                break

        return {
            'type': 'processing_result',
            'records_processed': base_records,
            'records_valid': int(base_records * random.uniform(0.8, 0.95)),
            'processing_time': time.time()
        }

    def _process_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Simule l'analyse de donnÃ©es."""
        time.sleep(random.uniform(2.0, 4.0))  # Simuler analyse complexe

        # Simuler une erreur occasionnelle
        if random.random() < 0.05:
            raise Exception("Erreur d'analyse")

        # Utiliser les donnÃ©es traitÃ©es si disponibles
        base_records = 400
        for key, value in data.items():
            if key.startswith('dep_') and isinstance(value, dict):
                base_records = value.get('records_valid', base_records)
                break

        return {
            'type': 'analysis_result',
            'insights_generated': random.randint(5, 15),
            'anomalies_detected': random.randint(0, 3),
            'confidence_score': random.uniform(0.7, 0.95),
            'analyzed_records': base_records
        }

    def _process_report_generation(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Simule la gÃ©nÃ©ration de rapport."""
        time.sleep(random.uniform(1.0, 2.0))  # Simuler gÃ©nÃ©ration

        # AgrÃ©ger les donnÃ©es des dÃ©pendances
        total_records = 0
        total_insights = 0

        for key, value in data.items():
            if key.startswith('dep_') and isinstance(value, dict):
                total_records += value.get('analyzed_records', 0)
                total_insights += value.get('insights_generated', 0)

        return {
            'type': 'report_result',
            'report_id': f"report_{int(time.time())}",
            'total_records_analyzed': total_records,
            'total_insights': total_insights,
            'report_size_mb': random.uniform(1.0, 10.0),
            'generated_at': time.time()
        }

    def _dependency_monitor(self):
        """Monitore les changements pour dÃ©bloquer de nouveaux jobs."""
        while self.active:
            self.job_completed_event.wait(timeout=2)
            self.job_completed_event.clear()

            with self.lock:
                self._check_ready_jobs()

    def _metrics_collector(self):
        """Collecte les mÃ©triques du systÃ¨me."""
        while self.active:
            time.sleep(10)  # Collecter toutes les 10 secondes

            with self.lock:
                stats = self.get_stats()

            self.logger.info(f"ğŸ“Š MÃ©triques: {stats['jobs_completed']} complÃ©tÃ©s, "
                           f"{stats['jobs_failed']} Ã©chouÃ©s, "
                           f"{stats['pending_jobs']} en attente")

    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du systÃ¨me."""
        with self.lock:
            stats = self.stats.copy()

            # Compter les jobs par statut
            status_counts = {}
            for status in self.job_status.values():
                status_counts[status.value] = status_counts.get(status.value, 0) + 1

            stats.update(status_counts)
            stats['pending_jobs'] = status_counts.get('pending', 0)
            stats['running_jobs'] = status_counts.get('running', 0)

            # Calculer les temps moyens
            if stats['jobs_completed'] > 0:
                stats['avg_processing_time'] = (
                    stats['total_processing_time'] / stats['jobs_completed']
                )
            else:
                stats['avg_processing_time'] = 0

            # Ã‰tat des queues
            queue_sizes = {}
            for job_type, job_queue in self.job_queues.items():
                queue_sizes[f'{job_type.value}_queue_size'] = job_queue.qsize()

            stats.update(queue_sizes)

            return stats

    def wait_completion(self, timeout: float = None) -> bool:
        """Attend que tous les jobs se terminent."""
        start_time = time.time()

        while self.active:
            with self.lock:
                pending = sum(1 for status in self.job_status.values()
                            if status in [JobStatus.PENDING, JobStatus.RUNNING])

                if pending == 0:
                    self.logger.info("âœ… Tous les jobs terminÃ©s")
                    return True

            if timeout and (time.time() - start_time) > timeout:
                self.logger.warning("â° Timeout atteint")
                return False

            time.sleep(1)

        return True

    def stop(self, timeout: float = 30):
        """ArrÃªte le systÃ¨me."""
        self.logger.info("ğŸ›‘ ArrÃªt du systÃ¨me...")
        self.active = False

        # Attendre que tous les workers se terminent
        all_workers = []
        for workers in self.workers.values():
            all_workers.extend(workers)

        for worker in all_workers:
            worker.join(timeout=timeout/len(all_workers))

        self.print_final_stats()

    def print_final_stats(self):
        """Affiche les statistiques finales."""
        stats = self.get_stats()

        print(f"\n{'='*50}")
        print("ğŸ“Š STATISTIQUES FINALES DU SYSTÃˆME")
        print(f"{'='*50}")
        print(f"Jobs soumis: {stats['jobs_submitted']}")
        print(f"Jobs complÃ©tÃ©s: {stats['jobs_completed']}")
        print(f"Jobs Ã©chouÃ©s: {stats['jobs_failed']}")
        print(f"Temps de traitement moyen: {stats['avg_processing_time']:.2f}s")

        success_rate = 0
        if stats['jobs_submitted'] > 0:
            success_rate = (stats['jobs_completed'] / stats['jobs_submitted']) * 100
        print(f"Taux de succÃ¨s: {success_rate:.1f}%")

        # DÃ©tail par statut
        print(f"\nRÃ©partition des jobs:")
        for status in JobStatus:
            count = stats.get(status.value, 0)
            if count > 0:
                print(f"  {status.value}: {count}")

def demo_distributed_system():
    """DÃ©monstration du systÃ¨me distribuÃ© complet."""
    print("ğŸŒ SYSTÃˆME DE TRAITEMENT DISTRIBUÃ‰")
    print("="*50)

    # CrÃ©er le systÃ¨me
    system = DistributedDataProcessor(
        ingestion_workers=2,
        processing_workers=2,
        analysis_workers=2,
        report_workers=1
    )

    try:
        # DÃ©marrer le systÃ¨me
        system.start()
        time.sleep(1)  # Laisser le systÃ¨me se stabiliser

        # CrÃ©er un pipeline de jobs interdÃ©pendants
        jobs = [
            # Phase 1: Ingestion de donnÃ©es (parallÃ¨le)
            Job("ingest_sales", JobType.DATA_INGESTION,
                {"source": "sales_db"}, priority=3),
            Job("ingest_customers", JobType.DATA_INGESTION,
                {"source": "customer_db"}, priority=3),
            Job("ingest_products", JobType.DATA_INGESTION,
                {"source": "product_db"}, priority=2),

            # Phase 2: Traitement (dÃ©pend de l'ingestion)
            Job("process_sales", JobType.DATA_PROCESSING,
                {"operation": "clean_sales"},
                dependencies=["ingest_sales"], priority=2),
            Job("process_customers", JobType.DATA_PROCESSING,
                {"operation": "clean_customers"},
                dependencies=["ingest_customers"], priority=2),

            # Phase 3: Analyse (dÃ©pend du traitement)
            Job("analyze_trends", JobType.DATA_ANALYSIS,
                {"analysis_type": "trend_analysis"},
                dependencies=["process_sales", "process_customers"], priority=1),
            Job("analyze_segments", JobType.DATA_ANALYSIS,
                {"analysis_type": "customer_segmentation"},
                dependencies=["process_customers"], priority=1),

            # Phase 4: GÃ©nÃ©ration de rapports (dÃ©pend de l'analyse)
            Job("generate_monthly_report", JobType.REPORT_GENERATION,
                {"report_type": "monthly_summary"},
                dependencies=["analyze_trends", "analyze_segments"], priority=1),
        ]

        # Soumettre tous les jobs
        print(f"\nğŸ“‹ Soumission de {len(jobs)} jobs...")
        for job in jobs:
            success = system.submit_job(job)
            if success:
                print(f"âœ… Job soumis: {job.id}")
            else:
                print(f"âŒ Ã‰chec soumission: {job.id}")
            time.sleep(0.2)

        # Surveiller le progrÃ¨s
        print(f"\nâ³ Traitement en cours...")

        # Attendre la completion avec timeout
        completed = system.wait_completion(timeout=60)

        if not completed:
            print("â° Certains jobs n'ont pas terminÃ© dans les temps")

    finally:
        # ArrÃªter le systÃ¨me
        system.stop()

if __name__ == "__main__":
    demo_distributed_system()
```

## Conclusion du Module 8

### RÃ©capitulatif des patterns appris

1. **Producer-Consumer** : Communication asynchrone dÃ©couplÃ©e
2. **Worker Pool** : ParallÃ©lisation de tÃ¢ches indÃ©pendantes
3. **Pipeline** : Traitement en Ã©tapes sÃ©quentielles
4. **Map-Reduce** : Traitement distribuÃ© de gros volumes
5. **Circuit Breaker** : Protection contre les dÃ©faillances
6. **Scatter-Gather** : RequÃªtes distribuÃ©es avec agrÃ©gation

### CompÃ©tences acquises

- âœ… **Analyser un problÃ¨me** de concurrence
- âœ… **Choisir le pattern appropriÃ©** selon le contexte
- âœ… **ImplÃ©menter des solutions robustes** avec gestion d'erreurs
- âœ… **GÃ©rer la synchronisation** et Ã©viter les race conditions
- âœ… **Monitorer et dÃ©boguer** du code concurrent
- âœ… **Combiner plusieurs patterns** dans un systÃ¨me complexe

### Projet final : Points clÃ©s

Le systÃ¨me distribuÃ© que nous avons construit dÃ©montre :

- **Architecture modulaire** avec workers spÃ©cialisÃ©s
- **Gestion des dÃ©pendances** entre jobs
- **Circuit breakers** pour la rÃ©silience
- **Monitoring et mÃ©triques** en temps rÃ©el
- **Retry et recovery** automatiques
- **ScalabilitÃ©** par type de tÃ¢che

### Conseils pour vos projets

1. **Commencez simple** : Un seul pattern Ã  la fois
2. **Mesurez d'abord** : Profilez avant d'optimiser
3. **GÃ©rez les erreurs** : La concurrence amplifie les bugs
4. **Testez sous charge** : Les bugs de concurrence sont sournois
5. **Documentez l'architecture** : Essentiel pour la maintenance

La programmation concurrente est un domaine complexe mais puissant. Avec les patterns et techniques que nous avons explorÃ©s, vous Ãªtes maintenant Ã©quipÃ©s pour crÃ©er des applications performantes et scalables !

ğŸ‰ **FÃ©licitations pour avoir terminÃ© le Module 8 sur la programmation concurrente !**

â­ï¸
